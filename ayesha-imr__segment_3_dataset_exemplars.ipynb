{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment 03: Dataset Exemplars\n",
    "\n",
    "## What Are Dataset Exemplars?\n",
    "\n",
    "In **Segment 02**, we used *activation maximization* to generate synthetic images that maximally activate specific neurons. Those visualizations show us a neuron's \"dream image\" — the ideal input pattern it's looking for.\n",
    "\n",
    "But synthetic images don't tell us what **real-world inputs** actually trigger those neurons.\n",
    "\n",
    "**Dataset exemplars** flip the approach:\n",
    "- Instead of *generating* images, we *search* through a large dataset of real images\n",
    "- For each neuron, we find the images that produce the highest activation\n",
    "- These show us what the neuron *actually responds to* in practice\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "| Approach | What It Shows | Limitation |\n",
    "|----------|--------------|------------|\n",
    "| Activation Maximization | The \"ideal\" input pattern | Synthetic, may not exist in real data |\n",
    "| Dataset Exemplars | What the neuron responds to in practice | Limited to images in the dataset |\n",
    "\n",
    "Together, they give a much richer picture of what each neuron has learned to detect.\n",
    "\n",
    "## What We'll Do\n",
    "\n",
    "1. Stream the full **ImageNet training set** (1.28 million images) from HuggingFace\n",
    "2. Pass each image through **InceptionV1** and capture activations at the `mixed4a` layer\n",
    "3. For each of the **first 10 channels**, track the **top 10 images** with highest activation\n",
    "4. **Checkpoint progress** to HuggingFace so we can pause/resume at any time\n",
    "5. Visualize and compare with our Segment 02 results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# - torch-lucent: For loading InceptionV1 (the old TensorFlow model ported to PyTorch)\n",
    "# - datasets: HuggingFace library for streaming ImageNet without downloading 150GB\n",
    "# - huggingface_hub: For saving/loading checkpoints to HuggingFace\n",
    "\n",
    "!pip install -q torch-lucent datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import heapq\n",
    "import json\n",
    "import io\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi, hf_hub_download, upload_file, create_repo\n",
    "from lucent.modelzoo import inceptionv1\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Authentication\n",
    "\n",
    "You need to be logged in to HuggingFace because:\n",
    "1. **ImageNet is gated** — you must accept the terms at https://huggingface.co/datasets/ILSVRC/imagenet-1k\n",
    "2. **We save checkpoints** to your private HF repo\n",
    "\n",
    "Run `huggingface-cli login` in your terminal, or use the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Interactive login (will prompt for token)\n",
    "# from huggingface_hub import login\n",
    "# login()\n",
    "\n",
    "# Option 2: If already logged in via CLI, this will confirm\n",
    "from huggingface_hub import whoami\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(f\"Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(\"Not logged in! Run: huggingface-cli login\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration\n",
    "\n",
    "All important settings in one place. **Edit the `HF_REPO_ID` to your own repository name.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Edit these values as needed\n",
    "# =============================================================================\n",
    "\n",
    "# HuggingFace repository for checkpoints\n",
    "# Format: \"your-username/your-repo-name\"\n",
    "HF_REPO_ID = \"ayesha-imr02/inceptionv1-imagenet-mixed4a-top10\"\n",
    "\n",
    "# Layer and channels to analyze\n",
    "LAYER_NAME = \"mixed4a\"   # The layer we're studying (middle layer of InceptionV1)\n",
    "NUM_CHANNELS = 10        # First 10 channels (matching Segment 02)\n",
    "TOP_K = 10               # Keep top 10 images per channel\n",
    "\n",
    "# Checkpointing\n",
    "CHECKPOINT_EVERY = 5000   # Save progress every N images\n",
    "CHECKPOINT_FILE = \"checkpoint.json\"  # Small metadata file (~5KB)\n",
    "\n",
    "# Processing\n",
    "TOTAL_IMAGES = 1_281_167  # ImageNet training set size\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  - HF Repository: {HF_REPO_ID}\")\n",
    "print(f\"  - Layer: {LAYER_NAME}\")\n",
    "print(f\"  - Channels: 0-{NUM_CHANNELS-1}\")\n",
    "print(f\"  - Top K images per channel: {TOP_K}\")\n",
    "print(f\"  - Checkpoint every: {CHECKPOINT_EVERY:,} images\")\n",
    "print(f\"  - Total images to process: {TOTAL_IMAGES:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Setup\n",
    "\n",
    "We load **InceptionV1** (also known as GoogLeNet or \"inception5h\") — the same model used in Segment 02.\n",
    "\n",
    "This is the original TensorFlow model from 2015, converted to PyTorch. It's commonly used in interpretability research because:\n",
    "- It has clear, well-studied features\n",
    "- The Distill article \"Feature Visualization\" provides reference visualizations\n",
    "- It's small enough to run quickly but deep enough to be interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect available device (GPU is much faster)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pretrained InceptionV1\n",
    "# - pretrained=True downloads weights trained on ImageNet\n",
    "# - .eval() puts the model in inference mode (disables dropout, etc.)\n",
    "model = inceptionv1(pretrained=True).to(device).eval()\n",
    "\n",
    "print(\"InceptionV1 loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Hook: Capturing Internal Activations\n",
    "\n",
    "To see what's happening *inside* the network, we use a PyTorch **forward hook**.\n",
    "\n",
    "**How hooks work:**\n",
    "1. We register a callback function on a specific layer (`mixed4a`)\n",
    "2. Every time data flows through that layer, our callback runs\n",
    "3. The callback saves the layer's output (the \"activations\") for us to analyze\n",
    "\n",
    "**About `mixed4a`:**\n",
    "- It's an \"Inception module\" — a block that applies multiple filter sizes in parallel\n",
    "- Output shape: `[batch, 508, H, W]` where 508 = 192 + 204 + 48 + 64 (from 4 branches)\n",
    "- H and W depend on input image size (for 224x224 input: H=W=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store captured activations\n",
    "# We use a dict so the hook function can modify it (closures can't reassign outer variables)\n",
    "activation_storage = {}\n",
    "\n",
    "def activation_hook(module, input_tensor, output_tensor):\n",
    "    \"\"\"\n",
    "    Forward hook callback.\n",
    "    \n",
    "    This function is called automatically every time the mixed4a layer\n",
    "    produces output during a forward pass.\n",
    "    \n",
    "    Args:\n",
    "        module: The layer this hook is attached to (mixed4a)\n",
    "        input_tensor: The input to this layer (we don't need it)\n",
    "        output_tensor: The layer's output — this is what we want!\n",
    "    \"\"\"\n",
    "    # Detach from computation graph (we don't need gradients)\n",
    "    # This saves memory and prevents gradient accumulation\n",
    "    activation_storage[LAYER_NAME] = output_tensor.detach()\n",
    "\n",
    "# Register the hook on the mixed4a layer\n",
    "# model.mixed4a is the Inception module we want to study\n",
    "hook_handle = model.mixed4a.register_forward_hook(activation_hook)\n",
    "\n",
    "print(f\"Hook registered on '{LAYER_NAME}'\")\n",
    "print(\"Now every forward pass will capture this layer's activations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Checkpoint Utilities\n",
    "\n",
    "Processing 1.28 million images takes several hours. We need robust checkpointing.\n",
    "\n",
    "**Approach:** Use counter-based filenames.\n",
    "\n",
    "```\n",
    "images/ch0_3847261.png → counter never changes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_repo_exists(repo_id):\n",
    "    \"\"\"\n",
    "    Create the HuggingFace repository if it doesn't exist.\n",
    "    \n",
    "    Args:\n",
    "        repo_id: Repository ID in format \"username/repo-name\"\n",
    "    \"\"\"\n",
    "    api = HfApi()\n",
    "    try:\n",
    "        api.repo_info(repo_id=repo_id, repo_type=\"dataset\")\n",
    "        print(f\"Repository '{repo_id}' already exists.\")\n",
    "    except Exception:\n",
    "        print(f\"Creating new private repository: {repo_id}\")\n",
    "        create_repo(repo_id=repo_id, repo_type=\"dataset\", private=True)\n",
    "        print(f\"Repository created!\")\n",
    "\n",
    "\n",
    "def upload_with_retry(api, content_bytes, path_in_repo, repo_id, max_retries=5):\n",
    "    \"\"\"\n",
    "    Upload a file to HuggingFace with retry logic.\n",
    "    \n",
    "    Args:\n",
    "        api: HfApi instance\n",
    "        content_bytes: File content as bytes\n",
    "        path_in_repo: Destination path in the repo\n",
    "        repo_id: Repository ID\n",
    "        max_retries: Number of retry attempts\n",
    "    \n",
    "    Returns:\n",
    "        True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            api.upload_file(\n",
    "                path_or_fileobj=content_bytes,\n",
    "                path_in_repo=path_in_repo,\n",
    "                repo_id=repo_id,\n",
    "                repo_type=\"dataset\",\n",
    "            )\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # Exponential backoff: 1s, 2s, 4s, 8s, 16s\n",
    "                print(f\"    Upload failed, retrying in {wait_time}s... ({e})\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"    Upload FAILED after {max_retries} attempts: {e}\")\n",
    "                return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_filename(channel, counter):\n",
    "    \"\"\"\n",
    "    Generate a STABLE filename for an image using its counter (unique ID).\n",
    "    \n",
    "    This is the key fix! Counter never changes, so filename never changes.\n",
    "    \n",
    "    Args:\n",
    "        channel: Channel number (0-9)\n",
    "        counter: Unique counter value for this image (assigned when it entered heap)\n",
    "    \n",
    "    Returns:\n",
    "        Filename like \"images/ch0_3847261.png\"\n",
    "    \"\"\"\n",
    "    return f\"images/ch{channel}_{counter}.png\"\n",
    "\n",
    "\n",
    "def upload_image(api, pil_image, channel, counter, repo_id):\n",
    "    \"\"\"\n",
    "    Upload a single image to HuggingFace with counter-based filename.\n",
    "    \n",
    "    Args:\n",
    "        api: HfApi instance\n",
    "        pil_image: PIL Image to upload\n",
    "        channel: Channel number\n",
    "        counter: Unique counter for this image\n",
    "        repo_id: Repository ID\n",
    "    \n",
    "    Returns:\n",
    "        Filename if successful, None otherwise\n",
    "    \"\"\"\n",
    "    filename = get_image_filename(channel, counter)\n",
    "    \n",
    "    # Convert PIL image to PNG bytes\n",
    "    buffer = io.BytesIO()\n",
    "    pil_image.save(buffer, format=\"PNG\")\n",
    "    image_bytes = buffer.getvalue()\n",
    "    \n",
    "    if upload_with_retry(api, image_bytes, filename, repo_id):\n",
    "        return filename\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(images_processed, top_images_heaps, repo_id, uploaded_images):\n",
    "    \"\"\"\n",
    "    Save current progress to HuggingFace.\n",
    "    \n",
    "    ONLY uploads NEW images (images not yet in uploaded_images dict).\n",
    "    Never re-uploads existing images, even if their rank changed.\n",
    "    \n",
    "    Args:\n",
    "        images_processed: Number of images we've processed so far\n",
    "        top_images_heaps: Dict mapping channel_id -> heap of (activation, counter, PIL_image)\n",
    "        repo_id: HuggingFace repository ID\n",
    "        uploaded_images: Set of (channel, counter) tuples that have been uploaded\n",
    "    \n",
    "    Returns:\n",
    "        Updated uploaded_images set\n",
    "    \"\"\"\n",
    "    api = HfApi()\n",
    "    \n",
    "    # Build checkpoint data and upload any NEW images\n",
    "    checkpoint_data = {\n",
    "        \"images_processed\": images_processed,\n",
    "        \"top_images\": {}\n",
    "    }\n",
    "    \n",
    "    images_uploaded_this_round = 0\n",
    "    upload_failures = 0\n",
    "    \n",
    "    for channel_id, heap in top_images_heaps.items():\n",
    "        channel_data = []\n",
    "        \n",
    "        for (activation, counter, pil_image) in heap:\n",
    "            # Check if this specific image has been uploaded\n",
    "            image_key = (channel_id, counter)\n",
    "            filename = get_image_filename(channel_id, counter)\n",
    "            \n",
    "            if image_key not in uploaded_images:\n",
    "                # NEW image — upload it\n",
    "                result = upload_image(api, pil_image, channel_id, counter, repo_id)\n",
    "                if result:\n",
    "                    uploaded_images.add(image_key)\n",
    "                    images_uploaded_this_round += 1\n",
    "                else:\n",
    "                    upload_failures += 1\n",
    "            \n",
    "            # Add to checkpoint data (regardless of upload success - we'll retry next time)\n",
    "            channel_data.append({\n",
    "                \"activation\": activation,\n",
    "                \"counter\": counter,\n",
    "                \"filename\": filename\n",
    "            })\n",
    "        \n",
    "        # Sort by activation (highest first) for readability in JSON\n",
    "        channel_data.sort(key=lambda x: x[\"activation\"], reverse=True)\n",
    "        checkpoint_data[\"top_images\"][str(channel_id)] = channel_data\n",
    "    \n",
    "    # Save the small JSON checkpoint\n",
    "    checkpoint_json = json.dumps(checkpoint_data, indent=2)\n",
    "    \n",
    "    if upload_with_retry(api, checkpoint_json.encode(\"utf-8\"), CHECKPOINT_FILE, repo_id):\n",
    "        status = f\"{images_processed:,} images | {images_uploaded_this_round} new uploads\"\n",
    "        if upload_failures > 0:\n",
    "            status += f\" | {upload_failures} failures\"\n",
    "        print(f\"  [Checkpoint: {status}]\")\n",
    "    else:\n",
    "        print(f\"  [WARNING: Checkpoint JSON upload failed at {images_processed:,} images]\")\n",
    "    \n",
    "    return uploaded_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(repo_id):\n",
    "    \"\"\"\n",
    "    Load previous progress from HuggingFace (if exists).\n",
    "    \n",
    "    Args:\n",
    "        repo_id: HuggingFace repository ID\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (images_processed, top_images_heaps, uploaded_images)\n",
    "        If no checkpoint exists, returns (0, empty_heaps, empty_set)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to download the checkpoint file\n",
    "        print(\"Downloading checkpoint...\")\n",
    "        checkpoint_path = hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=CHECKPOINT_FILE,\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "        \n",
    "        with open(checkpoint_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        images_processed = data[\"images_processed\"]\n",
    "        print(f\"Checkpoint found: {images_processed:,} images processed\")\n",
    "        \n",
    "        # Reconstruct the heaps and uploaded_images tracking\n",
    "        top_images_heaps = {}\n",
    "        uploaded_images = set()  # Changed to set for O(1) lookup\n",
    "        \n",
    "        print(\"Downloading top images...\")\n",
    "        for channel_str, entries in data[\"top_images\"].items():\n",
    "            channel_id = int(channel_str)\n",
    "            heap = []\n",
    "            \n",
    "            for entry in entries:\n",
    "                # Download the image from HuggingFace\n",
    "                try:\n",
    "                    image_path = hf_hub_download(\n",
    "                        repo_id=repo_id,\n",
    "                        filename=entry[\"filename\"],\n",
    "                        repo_type=\"dataset\"\n",
    "                    )\n",
    "                    pil_image = Image.open(image_path)\n",
    "                    # Convert to RGB if needed and make a copy to avoid file handle issues\n",
    "                    if pil_image.mode != \"RGB\":\n",
    "                        pil_image = pil_image.convert(\"RGB\")\n",
    "                    pil_image = pil_image.copy()\n",
    "                except Exception as e:\n",
    "                    print(f\"    Warning: Could not load {entry['filename']}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                heap_entry = (\n",
    "                    entry[\"activation\"],\n",
    "                    entry[\"counter\"],\n",
    "                    pil_image\n",
    "                )\n",
    "                heap.append(heap_entry)\n",
    "                \n",
    "                # Track that this image is already uploaded\n",
    "                uploaded_images.add((channel_id, entry[\"counter\"]))\n",
    "            \n",
    "            # Heapify to restore heap property\n",
    "            heapq.heapify(heap)\n",
    "            top_images_heaps[channel_id] = heap\n",
    "        \n",
    "        total_images_loaded = sum(len(h) for h in top_images_heaps.values())\n",
    "        print(f\"Checkpoint loaded! Resuming from image {images_processed:,}\")\n",
    "        print(f\"  Loaded {total_images_loaded} existing top images\")\n",
    "        return images_processed, top_images_heaps, uploaded_images\n",
    "    \n",
    "    except Exception as e:\n",
    "        # No checkpoint found — start fresh\n",
    "        print(f\"No checkpoint found. Starting from scratch.\")\n",
    "        print(f\"  (Reason: {e})\")\n",
    "        return 0, {ch: [] for ch in range(NUM_CHANNELS)}, set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ImageNet Streaming Setup\n",
    "\n",
    "The ImageNet training set is ~150GB. Instead of downloading it all, we **stream** images one at a time using HuggingFace's `datasets` library.\n",
    "\n",
    "**How streaming works:**\n",
    "- Images are downloaded on-demand as we iterate\n",
    "- Only one image is in memory at a time\n",
    "- Much faster to start (no waiting for full download)\n",
    "\n",
    "**Preprocessing for InceptionV1:**\n",
    "- Resize smallest edge to 256px, then center crop to 224×224\n",
    "- Scale pixel values: `pixel * 255 - 117` (the original TensorFlow model expects this range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing pipeline\n",
    "# This transforms PIL images into the format InceptionV1 expects\n",
    "\n",
    "preprocess_for_model = transforms.Compose([\n",
    "    transforms.Resize(256),           # Resize so smallest edge is 256px\n",
    "    transforms.CenterCrop(224),       # Crop center 224x224 region\n",
    "    transforms.ToTensor(),            # Convert to tensor, scales to [0, 1]\n",
    "])\n",
    "\n",
    "def preprocess_image(pil_image):\n",
    "    \"\"\"\n",
    "    Prepare an image for InceptionV1.\n",
    "    \n",
    "    Args:\n",
    "        pil_image: PIL Image (any size, RGB or other mode)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [1, 3, 224, 224] ready for the model\n",
    "    \"\"\"\n",
    "    # Ensure RGB (some ImageNet images are grayscale)\n",
    "    if pil_image.mode != \"RGB\":\n",
    "        pil_image = pil_image.convert(\"RGB\")\n",
    "    \n",
    "    # Apply transforms: resize, crop, convert to [0, 1] tensor\n",
    "    tensor = preprocess_for_model(pil_image)  # Shape: [3, 224, 224]\n",
    "    \n",
    "    # Scale for InceptionV1: [0, 1] -> [-117, 138]\n",
    "    # The original TF model was trained with this scaling\n",
    "    tensor = tensor * 255 - 117\n",
    "    \n",
    "    # Add batch dimension: [3, 224, 224] -> [1, 3, 224, 224]\n",
    "    return tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ImageNet training set in streaming mode\n",
    "# This requires you to have accepted the dataset terms on HuggingFace\n",
    "\n",
    "print(\"Loading ImageNet dataset (streaming mode)...\")\n",
    "print(\"Note: You must have accepted the terms at:\")\n",
    "print(\"  https://huggingface.co/datasets/ILSVRC/imagenet-1k\")\n",
    "\n",
    "imagenet_stream = load_dataset(\n",
    "    \"ILSVRC/imagenet-1k\",\n",
    "    split=\"train\",\n",
    "    streaming=True,  # Don't download everything — stream on demand\n",
    ")\n",
    "\n",
    "print(f\"ImageNet stream ready! Will process {TOTAL_IMAGES:,} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Main Processing Loop\n",
    "\n",
    "This is where the work happens. For each image:\n",
    "\n",
    "1. **Preprocess** — resize/crop/scale for InceptionV1\n",
    "2. **Forward pass** — run through the model (hook captures activations)\n",
    "3. **Compute channel activations** — mean activation per channel\n",
    "4. **Update heaps** — if this image is in the top-K for any channel, add it\n",
    "5. **Checkpoint** — save progress every 5000 images\n",
    "\n",
    "**About the heap data structure:**\n",
    "- We use a min-heap (smallest element on top) for efficiency\n",
    "- When full, we only keep an image if it beats the current minimum\n",
    "- This is O(log K) per update, much faster than sorting\n",
    "\n",
    "**You can safely interrupt this cell at any time!** Progress is saved every 5000 images. Just re-run the cell to resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure our HuggingFace repo exists (creates if needed)\n",
    "ensure_repo_exists(HF_REPO_ID)\n",
    "\n",
    "# Load checkpoint (or start fresh)\n",
    "images_processed, top_images, uploaded_images = load_checkpoint(HF_REPO_ID)\n",
    "\n",
    "# Counter for heap tie-breaking\n",
    "# (When two images have equal activation, we use this to decide order)\n",
    "# Start from where we left off to maintain consistency\n",
    "counter = images_processed * NUM_CHANNELS\n",
    "\n",
    "print(f\"\\nReady to process. Counter starting at: {counter:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Main processing loop\nprint(f\"\\nWill resume from image {images_processed:,}...\")\nprint(f\"Checkpoints will be saved every {CHECKPOINT_EVERY:,} images.\")\n\n# ============================================================================\n# Track actual dataset position (NOT enumerate index with tqdm initial offset!)\n# ============================================================================\ncurrent_idx = 0  # Actual position in the dataset (0 to 1.28M)\n\n# Progress bar shows total dataset (we'll skip the first images_processed quickly)\npbar = tqdm(\n    imagenet_stream,\n    total=TOTAL_IMAGES,\n    desc=\"Skipping\" if images_processed > 0 else \"Processing\"\n)\n\ntry:\n    with torch.no_grad():  # Disable gradient computation (we're only doing inference)\n        for sample in pbar:\n            \n            # Skip already-processed images (fast - no model inference)\n            if current_idx < images_processed:\n                current_idx += 1\n                # Show skip progress every 50,000 images\n                if current_idx % 50000 == 0:\n                    pbar.set_postfix({\"skipped\": f\"{current_idx:,}/{images_processed:,}\"})\n                continue\n            \n            # Change description once we start processing\n            if current_idx == images_processed:\n                pbar.set_description(\"Processing\")\n            \n            # Get the image (HuggingFace returns a dict with 'image' and 'label' keys)\n            pil_image = sample[\"image\"]\n\n            # Keep a copy of the original image (for storing in results)\n            original_image = pil_image.copy()\n\n            # Ensure RGB\n            if original_image.mode != \"RGB\":\n                original_image = original_image.convert(\"RGB\")\n\n            # Preprocess for InceptionV1\n            model_input = preprocess_image(pil_image).to(device)\n\n            # Forward pass — the hook automatically captures mixed4a activations\n            model(model_input)\n\n            # Get the captured activations\n            acts = activation_storage[LAYER_NAME]\n\n            # For each channel we're tracking\n            for ch in range(NUM_CHANNELS):\n                activation_value = acts[0, ch].mean().item()\n                entry = (activation_value, counter, original_image)\n                counter += 1\n\n                if len(top_images[ch]) < TOP_K:\n                    heapq.heappush(top_images[ch], entry)\n                elif activation_value > top_images[ch][0][0]:\n                    heapq.heapreplace(top_images[ch], entry)\n\n            # Increment position\n            current_idx += 1\n\n            # Update progress bar every 500 images\n            if current_idx % 500 == 0:\n                best_act = max(top_images[0])[0] if top_images[0] else 0\n                pbar.set_postfix({\"img\": f\"{current_idx:,}\", \"ch0_best\": f\"{best_act:.2f}\"})\n\n            # Save checkpoint periodically (based on ACTUAL position!)\n            if current_idx % CHECKPOINT_EVERY == 0:\n                uploaded_images = save_checkpoint(current_idx, top_images, HF_REPO_ID, uploaded_images)\n\n    # Final checkpoint\n    uploaded_images = save_checkpoint(TOTAL_IMAGES, top_images, HF_REPO_ID, uploaded_images)\n    print(\"\\nProcessing complete!\")\n\nexcept KeyboardInterrupt:\n    print(\"\\n\\nInterrupted! Saving checkpoint...\")\n    uploaded_images = save_checkpoint(current_idx, top_images, HF_REPO_ID, uploaded_images)\n    print(f\"Checkpoint saved at image {current_idx:,}. You can resume by re-running this cell.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Results Visualization\n",
    "\n",
    "Now let's see what we found! For each channel, we display the 10 ImageNet images that produced the highest activation.\n",
    "\n",
    "**How to interpret these results:**\n",
    "- Look for **common themes** across the top images for each channel\n",
    "- Compare with the **activation maximization** images from Segment 02\n",
    "- Some channels will be clearly interpretable; others may be more mysterious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization grid\n",
    "# Rows = channels (0-9), Columns = top images ranked by activation\n",
    "\n",
    "fig, axes = plt.subplots(NUM_CHANNELS, TOP_K, figsize=(20, 22))\n",
    "\n",
    "for ch in range(NUM_CHANNELS):\n",
    "    # Sort heap by activation (highest first)\n",
    "    # Heap entries are (activation, counter, pil_image)\n",
    "    ranked = sorted(top_images[ch], key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    for rank, (activation_value, _, pil_image) in enumerate(ranked):\n",
    "        ax = axes[ch][rank]\n",
    "        \n",
    "        # Display the image\n",
    "        ax.imshow(pil_image)\n",
    "        ax.axis(\"off\")\n",
    "        \n",
    "        # Add labels\n",
    "        if rank == 0:\n",
    "            # Channel label on the left\n",
    "            ax.set_ylabel(f\"Ch {ch}\", fontsize=14, rotation=0, labelpad=50, va=\"center\")\n",
    "            # Activation value below the image\n",
    "            ax.set_xlabel(f\"act={activation_value:.2f}\", fontsize=9)\n",
    "        else:\n",
    "            ax.set_xlabel(f\"{activation_value:.2f}\", fontsize=9)\n",
    "        \n",
    "        if ch == 0:\n",
    "            # Rank label on top\n",
    "            ax.set_title(f\"#{rank+1}\", fontsize=11)\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"Top-{TOP_K} ImageNet Images per Channel ({LAYER_NAME}, channels 0-{NUM_CHANNELS-1})\",\n",
    "    fontsize=16,\n",
    "    y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Observations & Analysis\n",
    "\n",
    "Look at each row (channel) and ask yourself:\n",
    "\n",
    "### Questions to Consider\n",
    "\n",
    "1. **Do the top images share a common theme?**\n",
    "   - If all top images for a channel contain similar content (e.g., furry textures, circular shapes, text), the neuron likely detects that pattern.\n",
    "\n",
    "2. **How does this compare to Segment 02?**\n",
    "   - The activation maximization showed the neuron's \"ideal\" input.\n",
    "   - Do these real images contain similar patterns, colors, or textures?\n",
    "   - Are there surprising differences?\n",
    "\n",
    "3. **Are some channels more interpretable?**\n",
    "   - Coherent top images → clear, monosemantic neuron\n",
    "   - Scattered, unrelated images → possibly **polysemantic** (responds to multiple concepts)\n",
    "\n",
    "4. **What specific features might each neuron detect?**\n",
    "   - Textures? (fur, scales, fabric)\n",
    "   - Shapes? (circles, curves, lines)\n",
    "   - Colors? (specific hues or contrasts)\n",
    "   - Objects? (eyes, wheels, faces)\n",
    "\n",
    "### Recording Your Observations\n",
    "\n",
    "Use the cell below to note what you see for each channel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations:**\n",
    "\n",
    "- **Channel 0**: _[What patterns do you see?]_\n",
    "- **Channel 1**: _[...]_\n",
    "- **Channel 2**: _[...]_\n",
    "- **Channel 3**: _[...]_\n",
    "- **Channel 4**: _[...]_\n",
    "- **Channel 5**: _[...]_\n",
    "- **Channel 6**: _[...]_\n",
    "- **Channel 7**: _[...]_\n",
    "- **Channel 8**: _[...]_\n",
    "- **Channel 9**: _[...]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the forward hook to clean up\n",
    "hook_handle.remove()\n",
    "print(\"Hook removed.\")\n",
    "\n",
    "# Clear activation storage\n",
    "activation_storage.clear()\n",
    "print(\"Activation storage cleared.\")\n",
    "\n",
    "print(f\"\\nResults are saved in your HuggingFace repo: {HF_REPO_ID}\")\n",
    "print(f\"You can view the images directly at: https://huggingface.co/datasets/{HF_REPO_ID}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}