{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Segment 03: Dataset Exemplars\n\n## What Are Dataset Exemplars?\n\nIn **Segment 02**, we used *activation maximization* to generate synthetic images that maximally activate specific neurons. Those visualizations show us a neuron's \"dream image\" — the ideal input pattern it's looking for.\n\nBut synthetic images don't tell us what **real-world inputs** actually trigger those neurons.\n\n**Dataset exemplars** flip the approach:\n- Instead of *generating* images, we *search* through a dataset of real images\n- For each neuron, we find the images that produce the highest activation\n- These show us what the neuron *actually responds to* in practice\n\n## Why This Matters\n\n| Approach | What It Shows | Limitation |\n|----------|--------------|------------|\n| Activation Maximization | The \"ideal\" input pattern | Synthetic, may not exist in real data |\n| Dataset Exemplars | What the neuron responds to in practice | Limited to images in the dataset |\n\nTogether, they give a much richer picture of what each neuron has learned to detect.\n\n## What We'll Do\n\n1. Stream the **ImageNet validation set** (50,000 images) from HuggingFace\n2. Pass each image through **InceptionV1** and capture activations at `mixed4a`\n3. For each of the **first 10 channels**, track the **top 10 images** with highest activation\n4. Visualize and compare with our Segment 02 results\n\n~3 minutes to complete, no checkpointing needed."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q torch-lucent datasets huggingface_hub"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport heapq\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms\nfrom datasets import load_dataset\nfrom lucent.modelzoo import inceptionv1\n\nprint(\"All imports successful!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### HuggingFace Authentication\n\nImageNet is a gated dataset. You must:\n1. Accept the terms at https://huggingface.co/datasets/ILSVRC/imagenet-1k\n2. Be logged in to HuggingFace\n\nRun `huggingface-cli login` in your terminal, or use the cell below:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Interactive login (will prompt for token)\n",
    "# from huggingface_hub import login\n",
    "# login()\n",
    "\n",
    "# Option 2: If already logged in via CLI, this will confirm\n",
    "from huggingface_hub import whoami\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(f\"Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(\"Not logged in! Run: huggingface-cli login\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Configuration"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# =============================================================================\n# CONFIGURATION\n# =============================================================================\n\n# Layer and channels to analyze\nLAYER_NAME = \"mixed4a\"   # The layer we're studying (middle layer of InceptionV1)\nNUM_CHANNELS = 10        # First 10 channels (matching Segment 02)\nTOP_K = 10               # Keep top 10 images per channel\n\n# Dataset\nDATASET_SPLIT = \"validation\"  # 50K images - manageable in one session!\nTOTAL_IMAGES = 50_000\n\nprint(\"Configuration:\")\nprint(f\"  - Dataset: ImageNet {DATASET_SPLIT} ({TOTAL_IMAGES:,} images)\")\nprint(f\"  - Layer: {LAYER_NAME}\")\nprint(f\"  - Channels: 0-{NUM_CHANNELS-1}\")\nprint(f\"  - Top K images per channel: {TOP_K}\")\nprint(f\"  - Estimated time: ~3 minutes\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Setup\n",
    "\n",
    "We load **InceptionV1** (also known as GoogLeNet or \"inception5h\") — the same model used in Segment 02.\n",
    "\n",
    "This is the original TensorFlow model from 2015, converted to PyTorch. It's commonly used in interpretability research because:\n",
    "- It has clear, well-studied features\n",
    "- The Distill article \"Feature Visualization\" provides reference visualizations\n",
    "- It's small enough to run quickly but deep enough to be interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect available device (GPU is much faster)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pretrained InceptionV1\n",
    "# - pretrained=True downloads weights trained on ImageNet\n",
    "# - .eval() puts the model in inference mode (disables dropout, etc.)\n",
    "model = inceptionv1(pretrained=True).to(device).eval()\n",
    "\n",
    "print(\"InceptionV1 loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Hook: Capturing Internal Activations\n",
    "\n",
    "To see what's happening *inside* the network, we use a PyTorch **forward hook**.\n",
    "\n",
    "**How hooks work:**\n",
    "1. We register a callback function on a specific layer (`mixed4a`)\n",
    "2. Every time data flows through that layer, our callback runs\n",
    "3. The callback saves the layer's output (the \"activations\") for us to analyze\n",
    "\n",
    "**About `mixed4a`:**\n",
    "- It's an \"Inception module\" — a block that applies multiple filter sizes in parallel\n",
    "- Output shape: `[batch, 508, H, W]` where 508 = 192 + 204 + 48 + 64 (from 4 branches)\n",
    "- H and W depend on input image size (for 224x224 input: H=W=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store captured activations\n",
    "# We use a dict so the hook function can modify it (closures can't reassign outer variables)\n",
    "activation_storage = {}\n",
    "\n",
    "def activation_hook(module, input_tensor, output_tensor):\n",
    "    \"\"\"\n",
    "    Forward hook callback.\n",
    "    \n",
    "    This function is called automatically every time the mixed4a layer\n",
    "    produces output during a forward pass.\n",
    "    \n",
    "    Args:\n",
    "        module: The layer this hook is attached to (mixed4a)\n",
    "        input_tensor: The input to this layer (we don't need it)\n",
    "        output_tensor: The layer's output — this is what we want!\n",
    "    \"\"\"\n",
    "    # Detach from computation graph (we don't need gradients)\n",
    "    # This saves memory and prevents gradient accumulation\n",
    "    activation_storage[LAYER_NAME] = output_tensor.detach()\n",
    "\n",
    "# Register the hook on the mixed4a layer\n",
    "# model.mixed4a is the Inception module we want to study\n",
    "hook_handle = model.mixed4a.register_forward_hook(activation_hook)\n",
    "\n",
    "print(f\"Hook registered on '{LAYER_NAME}'\")\n",
    "print(\"Now every forward pass will capture this layer's activations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. Data Structures\n\nWe use a **min-heap** to efficiently track the top-K images per channel.\n\n**Why a heap?**\n- Only stores K images (memory efficient)\n- O(log K) to check and update\n- Minimum element always on top — easy to compare and replace"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 5. ImageNet Validation Set\n\nWe use the **validation split** (50,000 images) instead of training (1.28M).\n\n- Completes in ~3 minutes\n- No checkpointing needed\n- Still enough images to find meaningful top activations\n\n**Preprocessing for InceptionV1:**\n- Resize smallest edge to 256px, then center crop to 224×224\n- Scale pixel values: `pixel * 255 - 117`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing pipeline\n",
    "# This transforms PIL images into the format InceptionV1 expects\n",
    "\n",
    "preprocess_for_model = transforms.Compose([\n",
    "    transforms.Resize(256),           # Resize so smallest edge is 256px\n",
    "    transforms.CenterCrop(224),       # Crop center 224x224 region\n",
    "    transforms.ToTensor(),            # Convert to tensor, scales to [0, 1]\n",
    "])\n",
    "\n",
    "def preprocess_image(pil_image):\n",
    "    \"\"\"\n",
    "    Prepare an image for InceptionV1.\n",
    "    \n",
    "    Args:\n",
    "        pil_image: PIL Image (any size, RGB or other mode)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [1, 3, 224, 224] ready for the model\n",
    "    \"\"\"\n",
    "    # Ensure RGB (some ImageNet images are grayscale)\n",
    "    if pil_image.mode != \"RGB\":\n",
    "        pil_image = pil_image.convert(\"RGB\")\n",
    "    \n",
    "    # Apply transforms: resize, crop, convert to [0, 1] tensor\n",
    "    tensor = preprocess_for_model(pil_image)  # Shape: [3, 224, 224]\n",
    "    \n",
    "    # Scale for InceptionV1: [0, 1] -> [-117, 138]\n",
    "    # The original TF model was trained with this scaling\n",
    "    tensor = tensor * 255 - 117\n",
    "    \n",
    "    # Add batch dimension: [3, 224, 224] -> [1, 3, 224, 224]\n",
    "    return tensor.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load ImageNet validation set (streaming - only 50K images)\nprint(\"Loading ImageNet validation set (streaming)...\")\nprint(\"Note: You must have accepted terms at:\")\nprint(\"  https://huggingface.co/datasets/ILSVRC/imagenet-1k\")\n\nimagenet_dataset = load_dataset(\n    \"ILSVRC/imagenet-1k\",\n    split=DATASET_SPLIT,\n    streaming=True,\n)\n\nprint(f\"Ready! Will process {TOTAL_IMAGES:,} images.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 6. Main Processing Loop\n\nSimple loop - no checkpointing needed for 50K images (~3 min).\n\nFor each image:\n1. Preprocess for InceptionV1\n2. Forward pass (hook captures activations)\n3. Update top-K heaps per channel"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize data structures\n# No checkpoint loading needed - we'll process everything in one go\n\ntop_images = {ch: [] for ch in range(NUM_CHANNELS)}  # Heaps for top-K per channel\ncounter = 0  # Tie-breaker for heap\n\nprint(\"Ready to process!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Process all 50K images\nprint(f\"Processing {TOTAL_IMAGES:,} images...\")\n\npbar = tqdm(imagenet_dataset, total=TOTAL_IMAGES, desc=\"Processing\")\n\nwith torch.no_grad():\n    for sample in pbar:\n        pil_image = sample[\"image\"]\n        \n        # Keep original for storage\n        original_image = pil_image.copy()\n        if original_image.mode != \"RGB\":\n            original_image = original_image.convert(\"RGB\")\n        \n        # Preprocess and run model\n        model_input = preprocess_image(pil_image).to(device)\n        model(model_input)\n        acts = activation_storage[LAYER_NAME]\n        \n        # Update heaps for each channel\n        for ch in range(NUM_CHANNELS):\n            activation_value = acts[0, ch].mean().item()\n            entry = (activation_value, counter, original_image)\n            counter += 1\n            \n            if len(top_images[ch]) < TOP_K:\n                heapq.heappush(top_images[ch], entry)\n            elif activation_value > top_images[ch][0][0]:\n                heapq.heapreplace(top_images[ch], entry)\n        \n        # Update progress bar\n        if counter % 5000 == 0:\n            best_act = max(top_images[0])[0] if top_images[0] else 0\n            pbar.set_postfix({\"ch0_best\": f\"{best_act:.2f}\"})\n\nprint(\"\\n✓ Done! Found top-10 images for each channel.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Results Visualization\n",
    "\n",
    "Now let's see what we found! For each channel, we display the 10 ImageNet images that produced the highest activation.\n",
    "\n",
    "**How to interpret these results:**\n",
    "- Look for **common themes** across the top images for each channel\n",
    "- Compare with the **activation maximization** images from Segment 02\n",
    "- Some channels will be clearly interpretable; others may be more mysterious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization grid\n",
    "# Rows = channels (0-9), Columns = top images ranked by activation\n",
    "\n",
    "fig, axes = plt.subplots(NUM_CHANNELS, TOP_K, figsize=(20, 22))\n",
    "\n",
    "for ch in range(NUM_CHANNELS):\n",
    "    # Sort heap by activation (highest first)\n",
    "    # Heap entries are (activation, counter, pil_image)\n",
    "    ranked = sorted(top_images[ch], key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    for rank, (activation_value, _, pil_image) in enumerate(ranked):\n",
    "        ax = axes[ch][rank]\n",
    "        \n",
    "        # Display the image\n",
    "        ax.imshow(pil_image)\n",
    "        ax.axis(\"off\")\n",
    "        \n",
    "        # Add labels\n",
    "        if rank == 0:\n",
    "            # Channel label on the left\n",
    "            ax.set_ylabel(f\"Ch {ch}\", fontsize=14, rotation=0, labelpad=50, va=\"center\")\n",
    "            # Activation value below the image\n",
    "            ax.set_xlabel(f\"act={activation_value:.2f}\", fontsize=9)\n",
    "        else:\n",
    "            ax.set_xlabel(f\"{activation_value:.2f}\", fontsize=9)\n",
    "        \n",
    "        if ch == 0:\n",
    "            # Rank label on top\n",
    "            ax.set_title(f\"#{rank+1}\", fontsize=11)\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"Top-{TOP_K} ImageNet Images per Channel ({LAYER_NAME}, channels 0-{NUM_CHANNELS-1})\",\n",
    "    fontsize=16,\n",
    "    y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Observations & Analysis\n",
    "\n",
    "Look at each row (channel) and ask yourself:\n",
    "\n",
    "### Questions to Consider\n",
    "\n",
    "1. **Do the top images share a common theme?**\n",
    "   - If all top images for a channel contain similar content (e.g., furry textures, circular shapes, text), the neuron likely detects that pattern.\n",
    "\n",
    "2. **How does this compare to Segment 02?**\n",
    "   - The activation maximization showed the neuron's \"ideal\" input.\n",
    "   - Do these real images contain similar patterns, colors, or textures?\n",
    "   - Are there surprising differences?\n",
    "\n",
    "3. **Are some channels more interpretable?**\n",
    "   - Coherent top images → clear, monosemantic neuron\n",
    "   - Scattered, unrelated images → possibly **polysemantic** (responds to multiple concepts)\n",
    "\n",
    "4. **What specific features might each neuron detect?**\n",
    "   - Textures? (fur, scales, fabric)\n",
    "   - Shapes? (circles, curves, lines)\n",
    "   - Colors? (specific hues or contrasts)\n",
    "   - Objects? (eyes, wheels, faces)\n",
    "\n",
    "### Recording Your Observations\n",
    "\n",
    "Use the cell below to note what you see for each channel:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your observations:**\n",
    "\n",
    "- **Channel 0**: _[What patterns do you see?]_\n",
    "- **Channel 1**: _[...]_\n",
    "- **Channel 2**: _[...]_\n",
    "- **Channel 3**: _[...]_\n",
    "- **Channel 4**: _[...]_\n",
    "- **Channel 5**: _[...]_\n",
    "- **Channel 6**: _[...]_\n",
    "- **Channel 7**: _[...]_\n",
    "- **Channel 8**: _[...]_\n",
    "- **Channel 9**: _[...]_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Remove the forward hook to clean up\nhook_handle.remove()\nprint(\"Hook removed.\")\n\n# Clear activation storage\nactivation_storage.clear()\nprint(\"Activation storage cleared.\")\n\nprint(\"\\nProcessing complete! Results are stored in the `top_images` dictionary.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}