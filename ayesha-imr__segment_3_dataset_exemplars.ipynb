{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment 03: Dataset Exemplars\n",
    "\n",
    "## What Are Dataset Exemplars?\n",
    "\n",
    "In **Segment 02**, we used *activation maximization* to generate synthetic images that maximally activate specific neurons. Those visualizations show us a neuron's \"dream image\" — the ideal input pattern it's looking for.\n",
    "\n",
    "But synthetic images don't tell us what **real-world inputs** actually trigger those neurons.\n",
    "\n",
    "**Dataset exemplars** flip the approach:\n",
    "- Instead of *generating* images, we *search* through a large dataset of real images\n",
    "- For each neuron, we find the images that produce the highest activation\n",
    "- These show us what the neuron *actually responds to* in practice\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "| Approach | What It Shows | Limitation |\n",
    "|----------|--------------|------------|\n",
    "| Activation Maximization | The \"ideal\" input pattern | Synthetic, may not exist in real data |\n",
    "| Dataset Exemplars | What the neuron responds to in practice | Limited to images in the dataset |\n",
    "\n",
    "Together, they give a much richer picture of what each neuron has learned to detect.\n",
    "\n",
    "## What We'll Do\n",
    "\n",
    "1. Stream the full **ImageNet training set** (1.28 million images) from HuggingFace\n",
    "2. Pass each image through **InceptionV1** and capture activations at the `mixed4a` layer\n",
    "3. For each of the **first 10 channels**, track the **top 10 images** with highest activation\n",
    "4. **Checkpoint progress** to HuggingFace so we can pause/resume at any time\n",
    "5. Visualize and compare with our Segment 02 results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "# - torch-lucent: For loading InceptionV1 (the old TensorFlow model ported to PyTorch)\n",
    "# - datasets: HuggingFace library for streaming ImageNet without downloading 150GB\n",
    "# - huggingface_hub: For saving/loading checkpoints to HuggingFace\n",
    "\n",
    "!pip install -q torch-lucent datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import heapq\n",
    "import json\n",
    "import base64\n",
    "import io\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import HfApi, hf_hub_download, upload_file, create_repo\n",
    "from lucent.modelzoo import inceptionv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HuggingFace Authentication\n",
    "\n",
    "You need to be logged in to HuggingFace because:\n",
    "1. **ImageNet is gated** — you must accept the terms at https://huggingface.co/datasets/ILSVRC/imagenet-1k\n",
    "2. **We save checkpoints** to your private HF repo\n",
    "\n",
    "Run `huggingface-cli login` in your terminal, or use the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Interactive login (will prompt for token)\n",
    "# from huggingface_hub import login\n",
    "# login()\n",
    "\n",
    "# Option 2: If already logged in via CLI, this will confirm\n",
    "from huggingface_hub import whoami\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(f\"Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(\"Not logged in! Run: huggingface-cli login\")\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Edit these values as needed\n",
    "# =============================================================================\n",
    "\n",
    "# HuggingFace repository for checkpoints (EDIT THIS!)\n",
    "# Format: \"your-username/your-repo-name\"\n",
    "HF_REPO_ID = \"ayesha-imr02/inceptionv1-imagenet-mixed4a-top10\"  # <-- CHANGE THIS!\n",
    "\n",
    "# Layer and channels to analyze\n",
    "LAYER_NAME = \"mixed4a\"   # The layer we're studying (middle layer of InceptionV1)\n",
    "NUM_CHANNELS = 10        # First 10 channels (matching Segment 02)\n",
    "TOP_K = 10               # Keep top 10 images per channel\n",
    "\n",
    "# Checkpointing\n",
    "CHECKPOINT_EVERY = 1000  # Save progress every N images (allows safe interruption)\n",
    "CHECKPOINT_FILE = \"checkpoint.json\"  # Filename in HF repo\n",
    "\n",
    "# Processing\n",
    "BATCH_SIZE = 1           # Process one image at a time (streaming mode)\n",
    "TOTAL_IMAGES = 1_281_167 # ImageNet training set size\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  - HF Repository: {HF_REPO_ID}\")\n",
    "print(f\"  - Layer: {LAYER_NAME}\")\n",
    "print(f\"  - Channels: 0-{NUM_CHANNELS-1}\")\n",
    "print(f\"  - Top K images per channel: {TOP_K}\")\n",
    "print(f\"  - Checkpoint every: {CHECKPOINT_EVERY} images\")\n",
    "print(f\"  - Total images to process: {TOTAL_IMAGES:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model Setup\n",
    "\n",
    "We load **InceptionV1** (also known as GoogLeNet or \"inception5h\") — the same model used in Segment 02.\n",
    "\n",
    "This is the original TensorFlow model from 2015, converted to PyTorch. It's commonly used in interpretability research because:\n",
    "- It has clear, well-studied features\n",
    "- The Distill article \"Feature Visualization\" provides reference visualizations\n",
    "- It's small enough to run quickly but deep enough to be interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect available device (GPU is much faster)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load pretrained InceptionV1\n",
    "# - pretrained=True downloads weights trained on ImageNet\n",
    "# - .eval() puts the model in inference mode (disables dropout, etc.)\n",
    "model = inceptionv1(pretrained=True).to(device).eval()\n",
    "\n",
    "print(\"InceptionV1 loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Hook: Capturing Internal Activations\n",
    "\n",
    "To see what's happening *inside* the network, we use a PyTorch **forward hook**.\n",
    "\n",
    "**How hooks work:**\n",
    "1. We register a callback function on a specific layer (`mixed4a`)\n",
    "2. Every time data flows through that layer, our callback runs\n",
    "3. The callback saves the layer's output (the \"activations\") for us to analyze\n",
    "\n",
    "**About `mixed4a`:**\n",
    "- It's an \"Inception module\" — a block that applies multiple filter sizes in parallel\n",
    "- Output shape: `[batch, 508, H, W]` where 508 = 192 + 204 + 48 + 64 (from 4 branches)\n",
    "- H and W depend on input image size (for 224x224 input: H=W=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store captured activations\n",
    "# We use a dict so the hook function can modify it (closures can't reassign outer variables)\n",
    "activation_storage = {}\n",
    "\n",
    "def activation_hook(module, input_tensor, output_tensor):\n",
    "    \"\"\"\n",
    "    Forward hook callback.\n",
    "    \n",
    "    This function is called automatically every time the mixed4a layer\n",
    "    produces output during a forward pass.\n",
    "    \n",
    "    Args:\n",
    "        module: The layer this hook is attached to (mixed4a)\n",
    "        input_tensor: The input to this layer (we don't need it)\n",
    "        output_tensor: The layer's output - this is what we want\n",
    "    \"\"\"\n",
    "    # Detach from computation graph (we don't need gradients)\n",
    "    # This saves memory and prevents gradient accumulation\n",
    "    activation_storage[LAYER_NAME] = output_tensor.detach()\n",
    "\n",
    "# Register the hook on the mixed4a layer\n",
    "# model.mixed4a is the Inception module we want to study\n",
    "hook_handle = model.mixed4a.register_forward_hook(activation_hook)\n",
    "\n",
    "print(f\"Hook registered on '{LAYER_NAME}'\")\n",
    "print(\"Now every forward pass will capture this layer's activations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Checkpoint Utilities\n",
    "\n",
    "Processing 1.28 million images takes several hours. If the runtime crashes or you need to stop, you don't want to start over.\n",
    "\n",
    "**Our checkpointing strategy:**\n",
    "1. Every 1000 images, save current progress to HuggingFace\n",
    "2. The checkpoint contains:\n",
    "   - `images_processed`: How many images we've seen\n",
    "   - `top_images`: The current top-K images for each channel (with their actual pixel data)\n",
    "3. On startup, we check if a checkpoint exists and resume from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_base64(pil_image):\n",
    "    \"\"\"\n",
    "    Convert a PIL Image to a base64-encoded string.\n",
    "    \n",
    "    We store images as base64 in JSON because:\n",
    "    - JSON is easy to save/load from HuggingFace\n",
    "    - Base64 preserves exact pixel values\n",
    "    - It's self-contained (no separate image files to track)\n",
    "    \n",
    "    Args:\n",
    "        pil_image: A PIL Image object\n",
    "    \n",
    "    Returns:\n",
    "        Base64-encoded string of the PNG image\n",
    "    \"\"\"\n",
    "    buffer = io.BytesIO()\n",
    "    pil_image.save(buffer, format=\"PNG\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def base64_to_image(b64_string):\n",
    "    \"\"\"\n",
    "    Convert a base64-encoded string back to a PIL Image.\n",
    "    \n",
    "    Args:\n",
    "        b64_string: Base64-encoded PNG image data\n",
    "    \n",
    "    Returns:\n",
    "        PIL Image object\n",
    "    \"\"\"\n",
    "    image_data = base64.b64decode(b64_string)\n",
    "    return Image.open(io.BytesIO(image_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_repo_exists(repo_id):\n",
    "    \"\"\"\n",
    "    Create the HuggingFace repository if it doesn't exist.\n",
    "    \n",
    "    Args:\n",
    "        repo_id: Repository ID in format \"username/repo-name\"\n",
    "    \"\"\"\n",
    "    api = HfApi()\n",
    "    try:\n",
    "        # Try to get repo info — if it exists, this succeeds\n",
    "        api.repo_info(repo_id=repo_id, repo_type=\"dataset\")\n",
    "        print(f\"Repository '{repo_id}' already exists.\")\n",
    "    except Exception:\n",
    "        # Repository doesn't exist — create it\n",
    "        print(f\"Creating new private repository: {repo_id}\")\n",
    "        create_repo(repo_id=repo_id, repo_type=\"dataset\", private=True)\n",
    "        print(f\"Repository created!\")\n",
    "\n",
    "\n",
    "def save_checkpoint(images_processed, top_images_heaps, repo_id):\n",
    "    \"\"\"\n",
    "    Save current progress to HuggingFace.\n",
    "    \n",
    "    This converts the in-memory heaps to a JSON-serializable format\n",
    "    and uploads to the HuggingFace repository.\n",
    "    \n",
    "    Args:\n",
    "        images_processed: Number of images we've processed so far\n",
    "        top_images_heaps: Dict mapping channel_id -> heap of (activation, counter, PIL_image)\n",
    "        repo_id: HuggingFace repository ID\n",
    "    \"\"\"\n",
    "    # Convert heaps to serializable format\n",
    "    # Each heap entry is (activation_value, tie_breaker_counter, pil_image)\n",
    "    serializable_data = {\n",
    "        \"images_processed\": images_processed,\n",
    "        \"top_images\": {}\n",
    "    }\n",
    "    \n",
    "    for channel_id, heap in top_images_heaps.items():\n",
    "        serializable_data[\"top_images\"][str(channel_id)] = [\n",
    "            {\n",
    "                \"activation\": entry[0],      # The activation value\n",
    "                \"counter\": entry[1],          # Tie-breaker counter\n",
    "                \"image_b64\": image_to_base64(entry[2])  # PIL image as base64\n",
    "            }\n",
    "            for entry in heap\n",
    "        ]\n",
    "    \n",
    "    # Write to a temporary file, then upload\n",
    "    checkpoint_json = json.dumps(serializable_data)\n",
    "    \n",
    "    # Upload to HuggingFace\n",
    "    api = HfApi()\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=checkpoint_json.encode(\"utf-8\"),\n",
    "        path_in_repo=CHECKPOINT_FILE,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\",\n",
    "    )\n",
    "    \n",
    "    print(f\"  [Checkpoint saved: {images_processed:,} images processed]\")\n",
    "\n",
    "\n",
    "def load_checkpoint(repo_id):\n",
    "    \"\"\"\n",
    "    Load previous progress from HuggingFace (if exists).\n",
    "    \n",
    "    Args:\n",
    "        repo_id: HuggingFace repository ID\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (images_processed, top_images_heaps)\n",
    "        If no checkpoint exists, returns (0, empty_heaps)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to download the checkpoint file\n",
    "        checkpoint_path = hf_hub_download(\n",
    "            repo_id=repo_id,\n",
    "            filename=CHECKPOINT_FILE,\n",
    "            repo_type=\"dataset\"\n",
    "        )\n",
    "        \n",
    "        with open(checkpoint_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # Reconstruct the heaps\n",
    "        top_images_heaps = {}\n",
    "        for channel_str, entries in data[\"top_images\"].items():\n",
    "            channel_id = int(channel_str)\n",
    "            heap = []\n",
    "            for entry in entries:\n",
    "                heap_entry = (\n",
    "                    entry[\"activation\"],\n",
    "                    entry[\"counter\"],\n",
    "                    base64_to_image(entry[\"image_b64\"])\n",
    "                )\n",
    "                heap.append(heap_entry)\n",
    "            # Heapify to restore heap property\n",
    "            heapq.heapify(heap)\n",
    "            top_images_heaps[channel_id] = heap\n",
    "        \n",
    "        images_processed = data[\"images_processed\"]\n",
    "        print(f\"Checkpoint loaded! Resuming from image {images_processed:,}\")\n",
    "        return images_processed, top_images_heaps\n",
    "    \n",
    "    except Exception as e:\n",
    "        # No checkpoint found — start fresh\n",
    "        print(f\"No checkpoint found. Starting from scratch.\")\n",
    "        print(f\"  (Reason: {e})\")\n",
    "        return 0, {ch: [] for ch in range(NUM_CHANNELS)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ImageNet Streaming Setup\n",
    "\n",
    "The ImageNet training set is ~150GB. Instead of downloading it all, we **stream** images one at a time using HuggingFace's `datasets` library.\n",
    "\n",
    "**How streaming works:**\n",
    "- Images are downloaded on-demand as we iterate\n",
    "- Only one image is in memory at a time\n",
    "- Much faster to start (no waiting for full download)\n",
    "\n",
    "**Preprocessing for InceptionV1:**\n",
    "- Resize smallest edge to 256px, then center crop to 224×224\n",
    "- Scale pixel values: `pixel * 255 - 117` (the original TensorFlow model expects this range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing pipeline\n",
    "# This transforms PIL images into the format InceptionV1 expects\n",
    "\n",
    "preprocess_for_model = transforms.Compose([\n",
    "    transforms.Resize(256),           # Resize so smallest edge is 256px\n",
    "    transforms.CenterCrop(224),       # Crop center 224x224 region\n",
    "    transforms.ToTensor(),            # Convert to tensor, scales to [0, 1]\n",
    "])\n",
    "\n",
    "def preprocess_image(pil_image):\n",
    "    \"\"\"\n",
    "    Prepare an image for InceptionV1.\n",
    "    \n",
    "    Args:\n",
    "        pil_image: PIL Image (any size, RGB or other mode)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of shape [1, 3, 224, 224] ready for the model\n",
    "    \"\"\"\n",
    "    # Ensure RGB (some ImageNet images are grayscale)\n",
    "    if pil_image.mode != \"RGB\":\n",
    "        pil_image = pil_image.convert(\"RGB\")\n",
    "    \n",
    "    # Apply transforms: resize, crop, convert to [0, 1] tensor\n",
    "    tensor = preprocess_for_model(pil_image)  # Shape: [3, 224, 224]\n",
    "    \n",
    "    # Scale for InceptionV1: [0, 1] -> [-117, 138]\n",
    "    # The original TF model was trained with this scaling\n",
    "    tensor = tensor * 255 - 117\n",
    "    \n",
    "    # Add batch dimension: [3, 224, 224] -> [1, 3, 224, 224]\n",
    "    return tensor.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ImageNet training set in streaming mode\n",
    "# This requires you to have accepted the dataset terms on HuggingFace\n",
    "\n",
    "imagenet_stream = load_dataset(\n",
    "    \"ILSVRC/imagenet-1k\",\n",
    "    split=\"train\",\n",
    "    streaming=True,  # Don't download everything — stream on demand\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(f\"ImageNet stream ready. Will process {TOTAL_IMAGES:,} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Main Processing Loop\n",
    "\n",
    "This is where the work happens. For each image:\n",
    "\n",
    "1. **Preprocess** — resize/crop/scale for InceptionV1\n",
    "2. **Forward pass** — run through the model (hook captures activations)\n",
    "3. **Compute channel activations** — mean activation per channel\n",
    "4. **Update heaps** — if this image is in the top-K for any channel, add it\n",
    "5. **Checkpoint** — save progress every 1000 images\n",
    "\n",
    "**About the heap data structure:**\n",
    "- We use a min-heap (smallest element on top) for efficiency\n",
    "- When full, we only keep an image if it beats the current minimum\n",
    "- This is O(log K) per update, much faster than sorting\n",
    "\n",
    "**You can safely interrupt this cell at any time!** Progress is saved every 1000 images. Just re-run the cell to resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure our HuggingFace repo exists (creates if needed)\n",
    "ensure_repo_exists(HF_REPO_ID)\n",
    "\n",
    "# Load checkpoint (or start fresh)\n",
    "images_processed, top_images = load_checkpoint(HF_REPO_ID)\n",
    "\n",
    "# Counter for heap tie-breaking\n",
    "# (When two images have equal activation, we use this to decide order)\n",
    "# Start from where we left off to maintain consistency\n",
    "counter = images_processed * NUM_CHANNELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing loop\n",
    "print(f\"\\nStarting processing from image {images_processed:,}...\")\n",
    "print(f\"Checkpoints will be saved every {CHECKPOINT_EVERY:,} images.\")\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(\n",
    "    enumerate(imagenet_stream),\n",
    "    total=TOTAL_IMAGES,\n",
    "    initial=images_processed,\n",
    "    desc=\"Processing ImageNet\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computation (we're only doing inference)\n",
    "    for idx, sample in pbar:\n",
    "        # Skip images we've already processed (when resuming)\n",
    "        if idx < images_processed:\n",
    "            continue\n",
    "        \n",
    "        # Get the image (HuggingFace returns a dict with 'image' and 'label' keys)\n",
    "        pil_image = sample[\"image\"]\n",
    "        \n",
    "        # Keep a copy of the original image (for storing in results)\n",
    "        # We store the original, not the preprocessed version\n",
    "        original_image = pil_image.copy()\n",
    "        \n",
    "        # Preprocess for InceptionV1\n",
    "        model_input = preprocess_image(pil_image).to(device)\n",
    "        \n",
    "        # Forward pass — the hook automatically captures mixed4a activations\n",
    "        model(model_input)\n",
    "        \n",
    "        # Get the captured activations\n",
    "        # Shape: [1, 508, H, W] where H=W=14 for 224x224 input\n",
    "        acts = activation_storage[LAYER_NAME]\n",
    "        \n",
    "        # For each channel we're tracking\n",
    "        for ch in range(NUM_CHANNELS):\n",
    "            # Compute mean spatial activation for this channel\n",
    "            # This tells us how strongly the whole image activates this channel\n",
    "            activation_value = acts[0, ch].mean().item()\n",
    "            \n",
    "            # Create heap entry: (activation, counter, image)\n",
    "            # The counter breaks ties when activations are equal\n",
    "            entry = (activation_value, counter, original_image)\n",
    "            counter += 1\n",
    "            \n",
    "            # Update the heap\n",
    "            if len(top_images[ch]) < TOP_K:\n",
    "                # Heap not full yet — just add the image\n",
    "                heapq.heappush(top_images[ch], entry)\n",
    "            elif activation_value > top_images[ch][0][0]:\n",
    "                # Heap is full, but this image beats the current minimum\n",
    "                # Replace the minimum with this new image\n",
    "                heapq.heapreplace(top_images[ch], entry)\n",
    "        \n",
    "        # Update progress bar with current best activation\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            best_act = max(top_images[0])[0] if top_images[0] else 0\n",
    "            pbar.set_postfix({\"ch0_best\": f\"{best_act:.2f}\"})\n",
    "        \n",
    "        # Save checkpoint periodically\n",
    "        if (idx + 1) % CHECKPOINT_EVERY == 0:\n",
    "            save_checkpoint(idx + 1, top_images, HF_REPO_ID)\n",
    "\n",
    "# Final checkpoint after processing all images\n",
    "save_checkpoint(TOTAL_IMAGES, top_images, HF_REPO_ID)\n",
    "print(\"\\nProcessing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Results Visualization\n",
    "\n",
    "Now let's see what we found! For each channel, we display the 10 ImageNet images that produced the highest activation.\n",
    "\n",
    "**How to interpret these results:**\n",
    "- Look for **common themes** across the top images for each channel\n",
    "- Compare with the **activation maximization** images from Segment 02\n",
    "- Some channels will be clearly interpretable; others may be more mysterious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization grid\n",
    "# Rows = channels (0-9), Columns = top images ranked by activation\n",
    "\n",
    "fig, axes = plt.subplots(NUM_CHANNELS, TOP_K, figsize=(20, 22))\n",
    "\n",
    "for ch in range(NUM_CHANNELS):\n",
    "    # Sort heap by activation (highest first)\n",
    "    # Heap entries are (activation, counter, pil_image)\n",
    "    ranked = sorted(top_images[ch], key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    for rank, (activation_value, _, pil_image) in enumerate(ranked):\n",
    "        ax = axes[ch][rank]\n",
    "        \n",
    "        # Display the image\n",
    "        ax.imshow(pil_image)\n",
    "        ax.axis(\"off\")\n",
    "        \n",
    "        # Add labels\n",
    "        if rank == 0:\n",
    "            # Channel label on the left\n",
    "            ax.set_ylabel(f\"Ch {ch}\", fontsize=14, rotation=0, labelpad=50, va=\"center\")\n",
    "            # Activation value below the image\n",
    "            ax.set_xlabel(f\"act={activation_value:.2f}\", fontsize=9)\n",
    "        else:\n",
    "            ax.set_xlabel(f\"{activation_value:.2f}\", fontsize=9)\n",
    "        \n",
    "        if ch == 0:\n",
    "            # Rank label on top\n",
    "            ax.set_title(f\"#{rank+1}\", fontsize=11)\n",
    "\n",
    "plt.suptitle(\n",
    "    f\"Top-{TOP_K} ImageNet Images per Channel ({LAYER_NAME}, channels 0–{NUM_CHANNELS-1})\",\n",
    "    fontsize=16,\n",
    "    y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the forward hook to clean up\n",
    "hook_handle.remove()\n",
    "print(\"Hook removed.\")\n",
    "\n",
    "# Clear activation storage\n",
    "activation_storage.clear()\n",
    "print(\"Activation storage cleared.\")\n",
    "\n",
    "print(f\"\\nResults are saved in HuggingFace repo: {HF_REPO_ID}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
