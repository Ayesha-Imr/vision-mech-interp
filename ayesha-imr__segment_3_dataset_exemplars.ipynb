{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment 03: Dataset Exemplars\n",
    "\n",
    "In Segment 02, we used **activation maximization** to generate synthetic images showing what neurons \"want\" to see. Those are useful, but they're artificial — they don't tell us what real-world inputs actually trigger those neurons.\n",
    "\n",
    "In this segment, we flip the approach: instead of synthesizing images, we **search through real ImageNet images** to find the ones that most strongly activate each neuron. These are called **dataset exemplars**.\n",
    "\n",
    "**Why this matters:**\n",
    "- Activation maximization shows the *ideal* input (a neuron's \"dream image\")\n",
    "- Dataset exemplars show what the neuron *actually responds to* in practice\n",
    "- Together, they give a much richer picture of what a neuron has learned\n",
    "\n",
    "**What we'll do:**\n",
    "- Pass ImageNet images through InceptionV1\n",
    "- Capture activations at the `mixed4a` layer using a forward hook\n",
    "- For each of the first 10 channels, find the 10 images that produce the highest activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch-lucent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import heapq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from lucent.modelzoo import inceptionv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained InceptionV1 (the old TF \"inception5h\" model ported to PyTorch)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = inceptionv1(pretrained=True).to(device).eval()\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capturing Activations with a Forward Hook\n",
    "\n",
    "To see what's happening *inside* the network, we use a PyTorch **forward hook**. This is a callback function that runs every time a specific layer processes data.\n",
    "\n",
    "We attach it to the `mixed4a` layer. After every forward pass, our hook captures and stores that layer's output (the activations) so we can inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the captured activation\n",
    "activation = {}\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    \"\"\"This function runs automatically every time mixed4a produces output.\n",
    "    It saves the output tensor so we can analyze it after the forward pass.\"\"\"\n",
    "    activation[\"mixed4a\"] = output.detach()\n",
    "\n",
    "# Register the hook on the mixed4a layer\n",
    "# model.mixed4a is a CatLayer that concatenates the 4 branches of the Inception module\n",
    "# Its output has 508 channels: 192 (1x1) + 204 (3x3) + 48 (5x5) + 64 (pool)\n",
    "hook_handle = model.mixed4a.register_forward_hook(hook_fn)\n",
    "\n",
    "print(\"Hook registered on mixed4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ImageNet\n",
    "\n",
    "We use the ImageNet validation set (50,000 images). Set the path below to point to your local copy.\n",
    "\n",
    "**Expected directory structure:**\n",
    "```\n",
    "imagenet_val_path/\n",
    "├── n01440764/\n",
    "│   ├── ILSVRC2012_val_00000293.JPEG\n",
    "│   └── ...\n",
    "├── n01443537/\n",
    "│   └── ...\n",
    "└── ...\n",
    "```\n",
    "\n",
    "**Preprocessing:** Lucent's InceptionV1 is the old TensorFlow model converted to PyTorch. It expects inputs scaled to the range [-117, 138] (i.e. pixel values in [0, 255] minus 117). We do this in two steps:\n",
    "1. Standard resize + crop + ToTensor → gives us [0, 1] range (for display)\n",
    "2. Scale by `x * 255 - 117` before feeding to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# SET YOUR IMAGENET PATH HERE\n",
    "# ========================\n",
    "IMAGENET_VAL_PATH = \"/path/to/imagenet/val\"  # <-- Change this!\n",
    "\n",
    "# Transforms: resize + center crop to 224x224, then convert to [0, 1] tensor.\n",
    "# We do NOT apply the InceptionV1 scaling here — we'll do that separately\n",
    "# so we can keep the [0,1] version for displaying images.\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),  # Converts PIL image to tensor in [0, 1] range\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.ImageFolder(IMAGENET_VAL_PATH, transform=preprocess)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Top-10 Activating Images for Each Channel\n",
    "\n",
    "For each image, we:\n",
    "1. Apply InceptionV1 preprocessing (`x * 255 - 117`)\n",
    "2. Run a forward pass (the hook automatically captures `mixed4a` output)\n",
    "3. For each of the first 10 channels, compute the **mean spatial activation** (average over the H×W spatial grid)\n",
    "4. Track the top-10 highest-activating images per channel using a min-heap\n",
    "\n",
    "**Why mean spatial activation?** Each channel's output is a 2D feature map (like a heatmap). Averaging over spatial positions tells us how strongly this image activates the channel *overall*, not just at one specific location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 10  # First 10 channels of mixed4a\n",
    "TOP_K = 10         # Top 10 images per channel\n",
    "\n",
    "# For each channel, maintain a min-heap of (activation_value, unique_id, image_tensor).\n",
    "# A min-heap keeps the smallest element on top, so we can efficiently check\n",
    "# whether a new image beats the current weakest entry.\n",
    "top_images = {ch: [] for ch in range(NUM_CHANNELS)}\n",
    "\n",
    "# Counter to break ties in the heap (avoids comparing tensors directly)\n",
    "counter = 0\n",
    "\n",
    "with torch.no_grad():  # No gradients needed — we're only doing inference\n",
    "    for images, _ in tqdm(dataloader, desc=\"Processing ImageNet\"):\n",
    "        # Apply InceptionV1 preprocessing: scale [0,1] → [-117, 138]\n",
    "        model_input = (images * 255 - 117).to(device)\n",
    "\n",
    "        # Forward pass — the hook captures mixed4a activations automatically\n",
    "        model(model_input)\n",
    "        acts = activation[\"mixed4a\"]  # Shape: [batch_size, 508, H, W]\n",
    "\n",
    "        # For each image in the batch\n",
    "        for i in range(images.size(0)):\n",
    "            for ch in range(NUM_CHANNELS):\n",
    "                # Mean activation across spatial dimensions for this channel\n",
    "                act_val = acts[i, ch].mean().item()\n",
    "\n",
    "                # Store the original [0,1] image (not the preprocessed one) for display later\n",
    "                entry = (act_val, counter, images[i].cpu())\n",
    "\n",
    "                if len(top_images[ch]) < TOP_K:\n",
    "                    # Heap not full yet — just add\n",
    "                    heapq.heappush(top_images[ch], entry)\n",
    "                elif act_val > top_images[ch][0][0]:\n",
    "                    # New image beats the weakest current top image — swap them\n",
    "                    heapq.heapreplace(top_images[ch], entry)\n",
    "\n",
    "                counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Results\n",
    "\n",
    "For each channel (0–9), we show the 10 ImageNet images that produced the highest mean activation. Images are sorted left-to-right from highest to lowest activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(NUM_CHANNELS, TOP_K, figsize=(20, 20))\n",
    "\n",
    "for ch in range(NUM_CHANNELS):\n",
    "    # Sort by activation (highest first)\n",
    "    ranked = sorted(top_images[ch], key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    for rank, (act_val, _, img_tensor) in enumerate(ranked):\n",
    "        ax = axes[ch][rank]\n",
    "        # Convert from [C, H, W] tensor to [H, W, C] numpy for matplotlib\n",
    "        img = img_tensor.permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img, 0, 1)  # Ensure valid range\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # Label: activation value on top image, channel label on leftmost image\n",
    "        if rank == 0:\n",
    "            ax.set_ylabel(f\"Ch {ch}\", fontsize=12, rotation=0, labelpad=40)\n",
    "        if ch == 0:\n",
    "            ax.set_title(f\"#{rank+1}\", fontsize=10)\n",
    "\n",
    "plt.suptitle(\"Top-10 ImageNet Images per Channel (mixed4a, channels 0–9)\", fontsize=16, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "Look at each row (channel) and ask yourself:\n",
    "\n",
    "- **Do the top images share a common theme?** If a channel's top images all contain, say, furry textures or circular shapes, the neuron likely detects that pattern.\n",
    "\n",
    "- **How does this compare to the activation maximization from Segment 02?** The synthetic visualization showed us the neuron's \"ideal\" input. Do these real images contain similar patterns, colors, or textures?\n",
    "\n",
    "- **Are some channels more interpretable than others?** Some channels may have clearly coherent top images (all showing the same kind of thing), while others may look more scattered. Scattered results could indicate a **polysemantic** neuron — one that responds to multiple unrelated concepts.\n",
    "\n",
    "- **What role does spatial structure play?** Some channels might respond to patterns regardless of where they appear; others might prefer certain spatial arrangements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Remove the forward hook so it doesn't interfere with future model use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook_handle.remove()\n",
    "print(\"Hook removed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
