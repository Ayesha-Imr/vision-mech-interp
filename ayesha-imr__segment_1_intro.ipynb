{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Ayesha-Imr/vision-mech-interp/blob/main/ayesha-imr__segment_1_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment 1: Building Intuition for CNNs\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Imagine you're trying to understand how a human brain recognizes faces. You wouldn't just look at the final \"yes, that's a face\" decision. You'd want to understand:\n",
    "- How the eyes detect edges and lines\n",
    "- How the brain combines those edges into shapes\n",
    "- How those shapes become \"nose,\" \"eyes,\" \"mouth\"\n",
    "- How all of that becomes \"face\"\n",
    "\n",
    "**CNNs work the same way.** Before we can \"interpret\" what a CNN is doing, we need to understand its basic mechanics.\n",
    "\n",
    "## The Mental Model We're Building\n",
    "\n",
    "By the end of this notebook, you'll understand:\n",
    "\n",
    "1. **Images aren't images to a CNN** ‚Äî they're arrays of numbers (tensors)\n",
    "2. **Convolutions are pattern matchers** ‚Äî each filter looks for one specific pattern\n",
    "3. **Depth creates abstraction** ‚Äî stacking simple operations creates complex understanding\n",
    "4. **Space is preserved** ‚Äî even deep in the network, there's spatial structure\n",
    "\n",
    "Without this foundation, interpretability techniques will feel like magic. With it, they'll make perfect sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Setup: Loading Our Tools\n",
    "\n",
    "### What We Need\n",
    "\n",
    "Think of this section as gathering your equipment before a lab experiment:\n",
    "- **PyTorch** ‚Äî the framework that lets us build and run neural networks\n",
    "- **VGG16** ‚Äî a pretrained CNN (someone already trained it on millions of images)\n",
    "- **An image** ‚Äî we'll use a cat photo from Wikipedia\n",
    "- **Visualization tools** ‚Äî matplotlib to see what's happening\n",
    "\n",
    "### Why VGG16?\n",
    "\n",
    "VGG16 is like a 1990s Toyota Camry:\n",
    "- Not the fanciest (ResNet, Vision Transformers are newer)\n",
    "- But simple, reliable, and easy to understand\n",
    "- Perfect for learning the fundamentals\n",
    "\n",
    "It has 16 layers organized into 5 \"blocks\" ‚Äî we'll explore these blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPORTS: The Libraries We Need\n",
    "# ============================================\n",
    "\n",
    "import torch                              # PyTorch: the neural network framework\n",
    "import torch.nn as nn                     # Neural network building blocks (layers, activations, etc.)\n",
    "import torchvision.models as models       # Pre-built models like VGG16, ResNet, etc.\n",
    "import torchvision.transforms as transforms  # Image preprocessing tools (resize, normalize, etc.)\n",
    "from PIL import Image                     # Python Imaging Library: load and manipulate images\n",
    "import matplotlib.pyplot as plt           # Plotting library: visualize images and graphs\n",
    "import numpy as np                        # Numerical computing: array operations\n",
    "import requests                           # Download images from URLs\n",
    "from io import BytesIO                    # Handle image data in memory (not saving to disk)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 1: Load the Pretrained Model\n",
    "# ============================================\n",
    "\n",
    "# What does \"pretrained\" mean?\n",
    "# - Someone (the PyTorch team) already trained this network on ImageNet\n",
    "# - ImageNet = 1.2 million images, 1000 categories (dogs, cats, cars, etc.)\n",
    "# - The network learned to recognize patterns through weeks of training on GPUs\n",
    "# - We're downloading those learned \"weights\" (the pattern detectors)\n",
    "\n",
    "model = models.vgg16(pretrained=True)  # Download and load VGG16 with pretrained weights\n",
    "\n",
    "# What does .eval() mean?\n",
    "# - Neural networks behave differently during training vs testing\n",
    "# - During training: they use dropout (randomly turn off neurons) and batch norm (normalize data)\n",
    "# - During evaluation: we turn these off for consistent, reproducible results\n",
    "# - .eval() puts the model in \"evaluation mode\"\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(\"‚úÖ VGG16 loaded and set to evaluation mode\")\n",
    "print(f\"\\nModel structure preview:\")\n",
    "print(f\"  - Total layers: {len(list(model.features))} convolutional layers\")\n",
    "print(f\"  - Input: 224√ó224√ó3 RGB image\")\n",
    "print(f\"  - Output: 1000 class probabilities (dog, cat, car, etc.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 2: Define Image Preprocessing\n",
    "# ============================================\n",
    "\n",
    "# Why do we need preprocessing?\n",
    "# - Raw images come in different sizes (640√ó480, 1920√ó1080, etc.)\n",
    "# - Pixel values are 0-255 (integers)\n",
    "# - VGG16 expects: exactly 224√ó224 pixels, normalized float values\n",
    "\n",
    "# Think of preprocessing as \"translating\" the image into the language the CNN speaks\n",
    "\n",
    "preprocess = transforms.Compose([  # Compose = chain multiple transformations together\n",
    "    \n",
    "    # Step 1: Resize the shorter side to 224 pixels (keeps aspect ratio)\n",
    "    # Example: 1200√ó800 image ‚Üí 224√ó149 image\n",
    "    transforms.Resize(224),\n",
    "    \n",
    "    # Step 2: Crop the center 224√ó224 square\n",
    "    # Example: 224√ó149 ‚Üí take center 224√ó224 (crops left/right edges)\n",
    "    transforms.CenterCrop(224),\n",
    "    \n",
    "    # Step 3: Convert PIL Image to PyTorch tensor\n",
    "    # - PIL stores as [height, width, channels] with values 0-255\n",
    "    # - Tensor stores as [channels, height, width] with values 0.0-1.0\n",
    "    # Example: [224, 224, 3] with 0-255 ‚Üí [3, 224, 224] with 0.0-1.0\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Step 4: Normalize using ImageNet statistics\n",
    "    # Why? VGG16 was trained on normalized images, so we must normalize too\n",
    "    # mean=[0.485, 0.456, 0.406] = average RGB values across all ImageNet images\n",
    "    # std=[0.229, 0.224, 0.225] = standard deviation of RGB values\n",
    "    # Formula: pixel_normalized = (pixel - mean) / std\n",
    "    # This centers the data around 0 and scales to similar ranges\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Preprocessing pipeline defined\")\n",
    "print(\"\\nWhat happens to an image:\")\n",
    "print(\"  1. Any size (e.g., 1200√ó800) ‚Üí 224√ó224 (resized and cropped)\")\n",
    "print(\"  2. [H, W, 3] uint8 0-255 ‚Üí [3, H, W] float32 0.0-1.0\")\n",
    "print(\"  3. Each channel normalized: (pixel - mean) / std\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 3: Load an Image from the Internet\n",
    "# ============================================\n",
    "\n",
    "def load_image(url):\n",
    "    \"\"\"\n",
    "    Download an image from a URL and return it as a PIL Image.\n",
    "    \n",
    "    Why we need the User-Agent header:\n",
    "    - Some websites block automated requests (bots)\n",
    "    - By pretending to be a web browser, we avoid getting blocked\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    \n",
    "    # Send HTTP GET request to download the image\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if download succeeded (status code 200 = success)\n",
    "    response.raise_for_status()  # Raises error if status is 4xx or 5xx\n",
    "    \n",
    "    # Convert the downloaded bytes into a PIL Image object\n",
    "    # BytesIO creates a file-like object in memory (no disk I/O)\n",
    "    # .convert('RGB') ensures we have 3 color channels (some images are grayscale)\n",
    "    img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Download a cat image from Wikimedia Commons\n",
    "img_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\"\n",
    "img = load_image(img_url)\n",
    "\n",
    "print(f\"‚úÖ Image downloaded successfully!\")\n",
    "print(f\"   Original size: {img.size}  # (width, height) in pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# STEP 4: Preprocess the Image\n",
    "# ============================================\n",
    "\n",
    "# Apply all the transformations we defined earlier\n",
    "input_tensor = preprocess(img)  # Now: [3, 224, 224] normalized tensor\n",
    "\n",
    "# Add a \"batch dimension\" at the front\n",
    "# Why? PyTorch processes images in batches for efficiency\n",
    "# - During training, you might process 32 images at once\n",
    "# - Here we only have 1 image, but we still need the batch dimension\n",
    "# - .unsqueeze(0) adds a dimension at position 0\n",
    "# - [3, 224, 224] ‚Üí [1, 3, 224, 224]\n",
    "#    ^new dimension (batch size = 1)\n",
    "\n",
    "input_tensor = input_tensor.unsqueeze(0)\n",
    "\n",
    "print(f\"‚úÖ Image preprocessed and ready for the network\")\n",
    "print(f\"\\nOriginal image size: {img.size}  # (width, height) = ({img.size[0]}√ó{img.size[1]})\")\n",
    "print(f\"Preprocessed tensor shape: {input_tensor.shape}\")\n",
    "print(f\"\\nBreaking down the shape [1, 3, 224, 224]:\")\n",
    "print(f\"  [0] Batch size = 1       # We're processing 1 image\")\n",
    "print(f\"  [1] Channels = 3         # Red, Green, Blue\")\n",
    "print(f\"  [2] Height = 224         # Rows of pixels\")\n",
    "print(f\"  [3] Width = 224          # Columns of pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the original image to see what we're working with\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Original Image: {img.size[0]}√ó{img.size[1]} pixels\", fontsize=14)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìå This is what the image looks like to us humans.\")\n",
    "print(\"   To the CNN, it's just a [1, 3, 224, 224] array of numbers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Code Block 1: First Contact ‚Äî Image ‚Üí Tensors ‚Üí Activations\n",
    "\n",
    "## The Big Question\n",
    "\n",
    "**What happens when an image enters a CNN?**\n",
    "\n",
    "## The Answer (Simplified)\n",
    "\n",
    "1. **Input:** `[1, 3, 224, 224]` ‚Äî one image with 3 color channels (RGB)\n",
    "2. **After first convolutional layer:** `[1, 64, 224, 224]` ‚Äî 64 \"feature maps\"\n",
    "3. **Each feature map:** Shows where a specific pattern was detected\n",
    "\n",
    "## The Analogy\n",
    "\n",
    "Imagine you have 64 different colored markers (red, blue, green, etc.).\n",
    "\n",
    "You look at the image and:\n",
    "- With marker #1, you highlight all vertical edges\n",
    "- With marker #2, you highlight all horizontal edges  \n",
    "- With marker #3, you highlight all diagonal edges\n",
    "- With marker #4, you highlight all circular shapes\n",
    "- ... and so on for all 64 markers\n",
    "\n",
    "Each marker gives you a **different highlighted version** of the same image. That's what the 64 feature maps are!\n",
    "\n",
    "## The Truth\n",
    "\n",
    "**The CNN doesn't \"see\" a cat.** It sees 64 different filtered versions, each responding to different local patterns (edges, textures, colors).\n",
    "\n",
    "## What We'll Do\n",
    "\n",
    "1. Extract the first convolutional layer from VGG16\n",
    "2. Pass our cat image through just that one layer\n",
    "3. Visualize the 64 resulting feature maps\n",
    "4. See how the \"cat\" is decomposed into pattern responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Extract the First Convolutional Layer\n",
    "# ============================================\n",
    "\n",
    "# VGG16 is organized as:\n",
    "# model.features = all convolutional and pooling layers (the \"feature extractor\")\n",
    "# model.classifier = fully connected layers at the end (the \"decision maker\")\n",
    "\n",
    "# model.features[0] = the very first layer\n",
    "# It's a Conv2d layer: Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "#   - Input: 3 channels (RGB)\n",
    "#   - Output: 64 channels (64 different pattern detectors)\n",
    "#   - Kernel size: 3√ó3 (each filter is a 3√ó3 grid)\n",
    "#   - Padding: 1 (adds 1-pixel border so output size = input size)\n",
    "\n",
    "first_conv = model.features[0]\n",
    "\n",
    "print(\"First layer details:\")\n",
    "print(first_conv)\n",
    "print(f\"\\nWhat this layer does:\")\n",
    "print(f\"  - Takes: [batch, 3, 224, 224] (3-channel image)\")\n",
    "print(f\"  - Returns: [batch, 64, 224, 224] (64 feature maps)\")\n",
    "print(f\"  - How: Applies 64 different 3√ó3 filters across the image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Pass the Image Through the First Layer\n",
    "# ============================================\n",
    "\n",
    "# torch.no_grad() context:\n",
    "# - During training, PyTorch tracks all operations to compute gradients (for backprop)\n",
    "# - We're not training, just visualizing, so we don't need gradients\n",
    "# - no_grad() turns off gradient tracking ‚Üí saves memory, runs faster\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Pass input through the first conv layer\n",
    "    # input_tensor: [1, 3, 224, 224]\n",
    "    # first_activations: [1, 64, 224, 224]\n",
    "    first_activations = first_conv(input_tensor)\n",
    "\n",
    "print(f\"‚úÖ Forward pass complete!\")\n",
    "print(f\"\\nInput shape:  {input_tensor.shape}  ‚Üê 3 color channels (R, G, B)\")\n",
    "print(f\"Output shape: {first_activations.shape}  ‚Üê 64 feature maps (pattern responses)\")\n",
    "print(f\"\\nWhat just happened:\")\n",
    "print(f\"  - 64 different 3√ó3 filters scanned the entire 224√ó224 image\")\n",
    "print(f\"  - Each filter detected a specific pattern (edges, textures, etc.)\")\n",
    "print(f\"  - Result: 64 activation maps showing WHERE each pattern was found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualize: Original Image vs Feature Maps\n",
    "# ============================================\n",
    "\n",
    "# Create a 2√ó5 grid of subplots\n",
    "# - 1st subplot: original image\n",
    "# - Next 9 subplots: first 9 feature maps (out of 64 total)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "# ----------------------------\n",
    "# Top-left: Original Image\n",
    "# ----------------------------\n",
    "axes[0, 0].imshow(img)  # Display the RGB image\n",
    "axes[0, 0].set_title(\"Original Image\\n(what we see)\", fontsize=10, fontweight='bold')\n",
    "axes[0, 0].axis('off')  # Hide x,y axis numbers\n",
    "\n",
    "# ----------------------------\n",
    "# Remaining 9 subplots: Feature Maps\n",
    "# ----------------------------\n",
    "for i in range(9):  # Show first 9 feature maps (out of 64)\n",
    "    # Calculate which subplot this is\n",
    "    # i=0: position 1 ‚Üí row=0, col=1\n",
    "    # i=1: position 2 ‚Üí row=0, col=2\n",
    "    # i=4: position 5 ‚Üí row=1, col=0  (wraps to next row)\n",
    "    row = (i + 1) // 5  # Integer division: which row?\n",
    "    col = (i + 1) % 5   # Modulo: which column?\n",
    "    \n",
    "    # Extract the i-th feature map\n",
    "    # first_activations shape: [1, 64, 224, 224]\n",
    "    #   [0] = batch index (we only have 1 image)\n",
    "    #   [i] = feature map index (0-63)\n",
    "    # Result: [224, 224] array of activation values\n",
    "    feature_map = first_activations[0, i].numpy()  # Convert to numpy for matplotlib\n",
    "    \n",
    "    # Display the feature map\n",
    "    # cmap='viridis': colormap (yellow=high activation, purple=low activation)\n",
    "    axes[row, col].imshow(feature_map, cmap='viridis')\n",
    "    axes[row, col].set_title(f\"Feature Map {i}\\n(filter {i} response)\", fontsize=9)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle(\"After First Conv Layer: One Image Becomes 64 Filtered Versions\", fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° WHAT YOU SHOULD SEE:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Each feature map looks different because each filter detects different patterns:\")\n",
    "print(\"  ‚Ä¢ Some highlight edges (bright lines where edges exist)\")\n",
    "print(\"  ‚Ä¢ Some highlight textures (fur, whiskers)\")\n",
    "print(\"  ‚Ä¢ Some respond to specific colors\")\n",
    "print(\"  ‚Ä¢ Some look almost random (that filter didn't find its pattern)\")\n",
    "print(\"\\n‚ö†Ô∏è The CNN doesn't 'see' a cat ‚Äî it sees 64 different pattern-responses!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaway from Block 1\n",
    "\n",
    "**CNNs transform images immediately.**\n",
    "\n",
    "- **Input:** A recognizable image (a cat)\n",
    "- **After 1 layer:** 64 abstract \"maps\" showing where patterns exist\n",
    "- **Each map:** Answers \"where does pattern X appear?\"\n",
    "\n",
    "This is **not** semantic understanding. It's **pattern matching**. The \"cat\" concept emerges much later, from combining these simple responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Code Block 2: Convolution as Local Pattern Detection\n",
    "\n",
    "## The Big Question\n",
    "\n",
    "**What ARE these \"filters\" and how do they work?**\n",
    "\n",
    "## The Intuition\n",
    "\n",
    "Imagine a stencil (like for painting letters on a wall):\n",
    "1. You place the stencil over a spot on the image\n",
    "2. You check: \"Does the pattern under the stencil match my stencil pattern?\"\n",
    "3. If yes ‚Üí high activation (bright spot on the feature map)\n",
    "4. If no ‚Üí low activation (dark spot)\n",
    "5. Move the stencil 1 pixel over and repeat\n",
    "\n",
    "That's exactly what a **convolutional filter** does!\n",
    "\n",
    "## The Math (Simplified)\n",
    "\n",
    "A 3√ó3 filter is just a 3√ó3 grid of numbers (\"weights\"):\n",
    "\n",
    "```\n",
    "Filter for detecting vertical edges:\n",
    "[-1,  0,  1]\n",
    "[-1,  0,  1]  \n",
    "[-1,  0,  1]\n",
    "```\n",
    "\n",
    "This filter:\n",
    "- Negative on the left (-1)\n",
    "- Zero in the middle (0)\n",
    "- Positive on the right (+1)\n",
    "\n",
    "When you slide it over an image:\n",
    "- **Vertical edge (dark‚Üíbright):** High positive response ‚úÖ\n",
    "- **Flat region (all same color):** Zero response ‚ùå\n",
    "- **Horizontal edge:** Zero response ‚ùå\n",
    "\n",
    "## Weight Sharing\n",
    "\n",
    "**Key insight:** The SAME filter is used everywhere in the image.\n",
    "\n",
    "- Top-left corner: filter checks for vertical edges\n",
    "- Bottom-right corner: SAME filter checks for vertical edges\n",
    "- This is why it's called \"weight sharing\" ‚Äî one set of weights, used everywhere\n",
    "\n",
    "## What We'll Do\n",
    "\n",
    "1. Look at the learned filter weights (the 3√ó3 patterns)\n",
    "2. See their corresponding activation maps\n",
    "3. Overlay activations on the original image to see WHERE patterns were detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Extract the Learned Filters\n",
    "# ============================================\n",
    "\n",
    "# Every Conv2d layer stores its filters in .weight\n",
    "# For first_conv (Conv2d(3, 64, kernel_size=3)):\n",
    "#   .weight.shape = [64, 3, 3, 3]\n",
    "#     [0] = 64 output channels (64 different filters)\n",
    "#     [1] = 3 input channels (R, G, B)\n",
    "#     [2] = 3 rows (3√ó3 kernel)\n",
    "#     [3] = 3 columns (3√ó3 kernel)\n",
    "\n",
    "filters = first_conv.weight.data.clone()  # .data = raw tensor, .clone() = make a copy\n",
    "\n",
    "print(f\"Filter tensor shape: {filters.shape}\")\n",
    "print(f\"\\nBreaking it down:\")\n",
    "print(f\"  [0] 64 filters        # 64 different pattern detectors\")\n",
    "print(f\"  [1] 3 input channels  # Each filter looks at R, G, B\")\n",
    "print(f\"  [2] 3 rows            # Kernel height\")\n",
    "print(f\"  [3] 3 columns         # Kernel width\")\n",
    "print(f\"\\nEach filter is a 3√ó3√ó3 cube of numbers.\")\n",
    "print(f\"Total parameters in this layer: {filters.numel()} = 64 √ó 3 √ó 3 √ó 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualize Filters and Their Activations\n",
    "# ============================================\n",
    "\n",
    "# We'll show 4 filters (out of 64) for clarity\n",
    "# For each filter, we show:\n",
    "#   Column 0: The filter itself (3√ó3 RGB pattern)\n",
    "#   Column 1: Arrow ‚Üí\n",
    "#   Column 2: Where that filter activates (the feature map)\n",
    "#   Column 3: Activation overlaid on original image\n",
    "\n",
    "fig, axes = plt.subplots(4, 6, figsize=(16, 10))\n",
    "\n",
    "for i in range(4):  # Show first 4 filters\n",
    "    \n",
    "    # =============================\n",
    "    # Column 0: The Filter Pattern\n",
    "    # =============================\n",
    "    \n",
    "    # Get filter i: shape [3, 3, 3] (RGB channels, 3√ó3 spatial)\n",
    "    filt = filters[i].permute(1, 2, 0).numpy()  # Permute to [3, 3, 3] for imshow\n",
    "    \n",
    "    # Normalize to [0, 1] for display\n",
    "    # Raw filter values can be negative or very large\n",
    "    # Normalization: (x - min) / (max - min) ‚Üí maps to [0, 1]\n",
    "    filt = (filt - filt.min()) / (filt.max() - filt.min() + 1e-8)  # +epsilon to avoid division by 0\n",
    "    \n",
    "    axes[i, 0].imshow(filt)\n",
    "    axes[i, 0].set_title(f\"Filter {i}\\n(3√ó3 RGB pattern)\", fontsize=9, fontweight='bold')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # =============================\n",
    "    # Column 1: Arrow (just visual)\n",
    "    # =============================\n",
    "    \n",
    "    axes[i, 1].text(0.5, 0.5, \"‚Üí\", fontsize=30, ha='center', va='center')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # =============================\n",
    "    # Column 2: Activation Map\n",
    "    # =============================\n",
    "    \n",
    "    # Get the corresponding activation map\n",
    "    # first_activations[0, i]: the i-th feature map, shape [224, 224]\n",
    "    activation = first_activations[0, i].numpy()\n",
    "    \n",
    "    # Display with 'hot' colormap (black=low, red=medium, yellow/white=high)\n",
    "    axes[i, 2].imshow(activation, cmap='hot')\n",
    "    axes[i, 2].set_title(f\"Where filter {i}\\nactivates strongly\", fontsize=9)\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    # =============================\n",
    "    # Column 3: Overlay on Original\n",
    "    # =============================\n",
    "    \n",
    "    # Show original image\n",
    "    axes[i, 3].imshow(img)\n",
    "    \n",
    "    # Resize activation map to match original image size (224√ó224 ‚Üí 1200√ó1198)\n",
    "    # Steps:\n",
    "    # 1. Scale activation to 0-255 uint8\n",
    "    # 2. Convert to PIL Image\n",
    "    # 3. Resize to original image dimensions\n",
    "    # 4. Convert back to numpy array\n",
    "    activation_resized = np.array(\n",
    "        Image.fromarray((activation * 255).astype(np.uint8)).resize(img.size)\n",
    "    )\n",
    "    \n",
    "    # Overlay as semi-transparent heatmap (alpha=0.5 = 50% transparent)\n",
    "    axes[i, 3].imshow(activation_resized, cmap='hot', alpha=0.5)\n",
    "    axes[i, 3].set_title(f\"Overlay:\\nBright = high activation\", fontsize=9)\n",
    "    axes[i, 3].axis('off')\n",
    "    \n",
    "    # =============================\n",
    "    # Columns 4-5: Empty (spacing)\n",
    "    # =============================\n",
    "    \n",
    "    axes[i, 4].axis('off')\n",
    "    axes[i, 5].axis('off')\n",
    "\n",
    "plt.suptitle(\"Filters as Pattern Detectors: Each 3√ó3 filter looks for a specific pattern\", \n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° WHAT YOU SHOULD SEE:\")\n",
    "print(\"=\"*60)\n",
    "print(\"Each row shows:\")\n",
    "print(\"  1. The filter pattern (3√ó3 RGB) ‚Äî this is what it's 'looking for'\")\n",
    "print(\"  2. The activation map ‚Äî WHERE that pattern was found\")\n",
    "print(\"  3. Overlay ‚Äî bright areas = strong match with the filter pattern\")\n",
    "print(\"\\nNotice:\")\n",
    "print(\"  ‚Ä¢ SAME filter detects patterns in MULTIPLE locations (weight sharing!)\")\n",
    "print(\"  ‚Ä¢ Different filters detect different patterns (edges, textures, colors)\")\n",
    "print(\"  ‚Ä¢ The 3√ó3 pattern is all that's needed to detect complex structures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaway from Block 2\n",
    "\n",
    "**Convolutions are sliding pattern matchers, not magic.**\n",
    "\n",
    "- Each filter = one 3√ó3 pattern to look for\n",
    "- Filter slides across the entire image (weight sharing)\n",
    "- High activation = \"I found my pattern here!\"\n",
    "- Low activation = \"My pattern is not here\"\n",
    "\n",
    "**This is local, not global.** Each filter only \"sees\" a 3√ó3 neighborhood at a time. Global understanding comes from stacking many layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Code Block 3: Depth = Abstraction\n",
    "\n",
    "## The Big Question\n",
    "\n",
    "**If filters only see 3√ó3 patches, how does a CNN understand whole objects?**\n",
    "\n",
    "## The Answer\n",
    "\n",
    "**Layers are hierarchical.** Each layer builds on the previous:\n",
    "\n",
    "```\n",
    "Layer 1 (early):   Edges, colors, simple textures\n",
    "         ‚Üì\n",
    "Layer 5 (middle):  Corners, curves, complex textures (combinations of edges)\n",
    "         ‚Üì  \n",
    "Layer 10 (deep):   Parts (eyes, wheels, fur patterns) ‚Äî combinations of curves/textures\n",
    "         ‚Üì\n",
    "Layer 16 (deeper): Whole objects (faces, cars) ‚Äî combinations of parts\n",
    "```\n",
    "\n",
    "## The Analogy\n",
    "\n",
    "Building a house:\n",
    "- **Layer 1:** Bricks (simple, local)\n",
    "- **Layer 5:** Walls (made from bricks)\n",
    "- **Layer 10:** Rooms (made from walls)\n",
    "- **Layer 16:** House (made from rooms)\n",
    "\n",
    "Each layer sees **more context** (larger receptive field) and learns **more abstract concepts**.\n",
    "\n",
    "## What Changes with Depth\n",
    "\n",
    "| Property | Early Layers | Middle Layers | Deep Layers |\n",
    "|----------|-------------|---------------|-------------|\n",
    "| **Spatial resolution** | High (224√ó224) | Medium (56√ó56) | Low (14√ó14) |\n",
    "| **Number of channels** | Few (64) | Medium (256) | Many (512) |\n",
    "| **Features detected** | Edges, colors | Textures, patterns | Object parts, concepts |\n",
    "| **Receptive field** | 3√ó3 pixels | ~40√ó40 pixels | ~200√ó200 pixels |\n",
    "| **Activation sparsity** | Dense (many active) | Medium | Sparse (few active) |\n",
    "\n",
    "## What We'll Do\n",
    "\n",
    "1. Extract activations from three depths: early, middle, deep\n",
    "2. Compare their shapes and visual appearance\n",
    "3. See how abstraction increases with depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Set Up Hooks to Capture Activations\n",
    "# ============================================\n",
    "\n",
    "# Problem: We want to see activations from MIDDLE of the network, not just output\n",
    "# Solution: \"Hooks\" ‚Äî callbacks that run during the forward pass\n",
    "\n",
    "# Dictionary to store captured activations\n",
    "activations = {}\n",
    "\n",
    "# Function factory: creates a hook function that saves to a specific name\n",
    "def get_activation(name):\n",
    "    \"\"\"Returns a hook function that captures activations.\"\"\"\n",
    "    def hook(model, input, output):\n",
    "        # This function runs DURING the forward pass\n",
    "        # model: the layer being executed\n",
    "        # input: what went into the layer\n",
    "        # output: what came out of the layer\n",
    "        \n",
    "        # .detach() = disconnect from computation graph (we're not training)\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# VGG16 has 5 blocks of conv layers:\n",
    "# Block 1: features[0-4]   (64 channels)\n",
    "# Block 2: features[5-9]   (128 channels)\n",
    "# Block 3: features[10-16] (256 channels)\n",
    "# Block 4: features[17-23] (512 channels)\n",
    "# Block 5: features[24-30] (512 channels)\n",
    "\n",
    "# Register hooks at three different depths\n",
    "model.features[2].register_forward_hook(get_activation('early'))    # End of block 1\n",
    "model.features[16].register_forward_hook(get_activation('middle'))  # End of block 3\n",
    "model.features[28].register_forward_hook(get_activation('deep'))    # End of block 5\n",
    "\n",
    "print(\"‚úÖ Hooks registered at three depths:\")\n",
    "print(f\"   ‚Ä¢ Layer 2 (early):  After first conv block\")\n",
    "print(f\"   ‚Ä¢ Layer 16 (middle): After third conv block\")\n",
    "print(f\"   ‚Ä¢ Layer 28 (deep):   Near the end of conv layers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Run Forward Pass to Capture Activations\n",
    "# ============================================\n",
    "\n",
    "with torch.no_grad():  # No gradients needed\n",
    "    _ = model(input_tensor)  # Full forward pass through VGG16\n",
    "    # The hooks captured activations automatically!\n",
    "\n",
    "# Check what we captured\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ACTIVATION SHAPES AT DIFFERENT DEPTHS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, act in activations.items():\n",
    "    print(f\"\\n{name.upper()} layer:\")\n",
    "    print(f\"  Shape: {act.shape}\")\n",
    "    print(f\"  Breakdown: [batch={act.shape[0]}, channels={act.shape[1]}, height={act.shape[2]}, width={act.shape[3]}]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTICE THE PATTERN:\")\n",
    "print(\"=\"*60)\n",
    "print(\"As we go deeper:\")\n",
    "print(\"  ‚úì Spatial dimensions DECREASE (224 ‚Üí 112 ‚Üí 28 ‚Üí 14)\")\n",
    "print(\"    Why? Pooling layers downsample\")\n",
    "print(\"\\n  ‚úì Number of channels INCREASES (64 ‚Üí 256 ‚Üí 512)\")\n",
    "print(\"    Why? More features to detect more complex patterns\")\n",
    "print(\"\\nThis is the trade-off: spatial resolution ‚Üî semantic abstraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualize Feature Maps from Each Depth\n",
    "# ============================================\n",
    "\n",
    "# We'll show 4 feature maps from each depth\n",
    "# Strategy: Pick the most \"interesting\" channels (highest variance = most informative)\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(16, 10))\n",
    "\n",
    "# Iterate through depths (row 0 = early, row 1 = middle, row 2 = deep)\n",
    "for row, (name, act) in enumerate(activations.items()):\n",
    "    \n",
    "    # =============================\n",
    "    # Column 0: Label\n",
    "    # =============================\n",
    "    \n",
    "    axes[row, 0].text(0.5, 0.5, f\"{name.upper()}\\nLayer\", \n",
    "                     fontsize=14, ha='center', va='center', fontweight='bold')\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    # =============================\n",
    "    # Find Most Interesting Channels\n",
    "    # =============================\n",
    "    \n",
    "    # Variance measures how much a feature map varies\n",
    "    # High variance = informative (has structure)\n",
    "    # Low variance = boring (mostly uniform)\n",
    "    \n",
    "    # act[0] = first (only) image in batch, shape [channels, H, W]\n",
    "    # .var(dim=[1, 2]) = variance across spatial dimensions (H, W)\n",
    "    # Result: [channels] ‚Äî one variance value per channel\n",
    "    variances = act[0].var(dim=[1, 2])\n",
    "    \n",
    "    # Get indices of top 4 channels with highest variance\n",
    "    top_channels = torch.argsort(variances, descending=True)[:4]\n",
    "    \n",
    "    # =============================\n",
    "    # Columns 1-4: Feature Maps\n",
    "    # =============================\n",
    "    \n",
    "    for col, ch in enumerate(top_channels):\n",
    "        # Extract feature map for channel ch\n",
    "        feature_map = act[0, ch].numpy()\n",
    "        \n",
    "        # Display with viridis colormap\n",
    "        axes[row, col + 1].imshow(feature_map, cmap='viridis')\n",
    "        axes[row, col + 1].set_title(f\"Channel {ch.item()}\\n({feature_map.shape[0]}√ó{feature_map.shape[1]})\", \n",
    "                                     fontsize=9)\n",
    "        axes[row, col + 1].axis('off')\n",
    "\n",
    "# Add annotations explaining what we see\n",
    "axes[0, 4].text(1.2, 0.5, \n",
    "                \"‚Üê EARLY\\n   ‚Ä¢ High resolution\\n   ‚Ä¢ Edge-like\\n   ‚Ä¢ Dense activations\", \n",
    "                fontsize=10, transform=axes[0, 4].transAxes, va='center')\n",
    "\n",
    "axes[1, 4].text(1.2, 0.5, \n",
    "                \"‚Üê MIDDLE\\n   ‚Ä¢ Medium resolution\\n   ‚Ä¢ Texture patterns\\n   ‚Ä¢ More structured\", \n",
    "                fontsize=10, transform=axes[1, 4].transAxes, va='center')\n",
    "\n",
    "axes[2, 4].text(1.2, 0.5, \n",
    "                \"‚Üê DEEP\\n   ‚Ä¢ Low resolution\\n   ‚Ä¢ Abstract, sparse\\n   ‚Ä¢ Object-level\", \n",
    "                fontsize=10, transform=axes[2, 4].transAxes, va='center')\n",
    "\n",
    "plt.suptitle(\"Depth = Abstraction: Features become more abstract deeper in the network\", \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° WHAT YOU SHOULD SEE:\")\n",
    "print(\"=\"*60)\n",
    "print(\"EARLY (row 1):\")\n",
    "print(\"  ‚Ä¢ High resolution (224√ó224 or similar)\")\n",
    "print(\"  ‚Ä¢ Looks like filtered versions of the image\")\n",
    "print(\"  ‚Ä¢ You can still recognize the cat's shape\")\n",
    "print(\"  ‚Ä¢ Detects: edges, simple textures\")\n",
    "print(\"\\nMIDDLE (row 2):\")\n",
    "print(\"  ‚Ä¢ Medium resolution (~56√ó56)\")\n",
    "print(\"  ‚Ä¢ More abstract patterns\")\n",
    "print(\"  ‚Ä¢ Harder to see the original image\")\n",
    "print(\"  ‚Ä¢ Detects: complex textures, motifs\")\n",
    "print(\"\\nDEEP (row 3):\")\n",
    "print(\"  ‚Ä¢ Very low resolution (~14√ó14)\")\n",
    "print(\"  ‚Ä¢ Mostly uniform or very sparse\")\n",
    "print(\"  ‚Ä¢ Only a few 'hot spots' of activation\")\n",
    "print(\"  ‚Ä¢ Detects: high-level concepts, object parts\")\n",
    "print(\"\\n‚ö†Ô∏è This is WHY interpretability must be LAYER-AWARE!\")\n",
    "print(\"   Different layers show completely different information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaway from Block 3\n",
    "\n",
    "**Abstraction emerges from depth, not from any single smart layer.**\n",
    "\n",
    "- **Early layers:** See 3√ó3 patches ‚Üí detect edges\n",
    "- **Middle layers:** Combine edges ‚Üí detect textures and patterns  \n",
    "- **Deep layers:** Combine patterns ‚Üí detect object parts and concepts\n",
    "\n",
    "**The magic is in the composition.** Simple operations (convolution, pooling) stacked many times create complex understanding.\n",
    "\n",
    "**Implication for interpretability:** If you want to understand \"what the network sees,\" you must specify WHICH LAYER. Early layers show low-level features, deep layers show high-level concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Code Block 4: Spatial Locality & Receptive Fields\n",
    "\n",
    "## The Big Question\n",
    "\n",
    "**If deep layers have such low resolution (14√ó14), how do they still correspond to locations in the original image?**\n",
    "\n",
    "## The Answer: Receptive Fields\n",
    "\n",
    "Each neuron in a deep layer doesn't \"see\" one pixel. It sees a **receptive field** ‚Äî a region of the original image.\n",
    "\n",
    "```\n",
    "Layer 1 neuron:  sees 3√ó3 pixels\n",
    "Layer 5 neuron:  sees ~40√ó40 pixels (accumulated through layers)\n",
    "Layer 15 neuron: sees ~200√ó200 pixels (almost the whole image!)\n",
    "```\n",
    "\n",
    "But even though receptive fields grow, **spatial correspondence is maintained**:\n",
    "- Top-left neuron in layer 15 ‚Üí corresponds to top-left region of image\n",
    "- Bottom-right neuron in layer 15 ‚Üí corresponds to bottom-right region\n",
    "\n",
    "## The Experiment\n",
    "\n",
    "To prove this, we'll:\n",
    "1. Black out a region of the image (the cat's face)\n",
    "2. Compare activations: original vs perturbed\n",
    "3. See where the differences appear\n",
    "\n",
    "**Hypothesis:** Changes should appear in corresponding spatial locations across all layers.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "This is why techniques like **Grad-CAM** and **saliency maps** work!\n",
    "- They rely on the fact that deep layers still have spatial structure\n",
    "- We can trace activations back to specific image regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Create a Perturbed Version of the Image\n",
    "# ============================================\n",
    "\n",
    "# Make a copy of the original tensor (don't modify the original!)\n",
    "perturbed_tensor = input_tensor.clone()\n",
    "\n",
    "# Black out a rectangular region\n",
    "# Tensor shape: [1, 3, 224, 224]\n",
    "#   [:, :, 50:150, 60:160] means:\n",
    "#     : = all batches (just 1)\n",
    "#     : = all channels (R, G, B)\n",
    "#     50:150 = rows 50-149 (height)\n",
    "#     60:160 = columns 60-159 (width)\n",
    "# Setting to 0 = black (after normalization, 0 is not quite black, but close)\n",
    "\n",
    "perturbed_tensor[:, :, 50:150, 60:160] = 0\n",
    "\n",
    "print(\"‚úÖ Created perturbed image with blacked-out region\")\n",
    "print(f\"   Region: rows 50-150, columns 60-160\")\n",
    "print(f\"   Size of blackout: 100√ó100 pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Capture Activations for Both Images\n",
    "# ============================================\n",
    "\n",
    "# We need fresh storage dictionaries\n",
    "activations_original = {}\n",
    "activations_perturbed = {}\n",
    "\n",
    "# Hook factory (same as before, but stores to different dict)\n",
    "def get_activation_dict(storage, name):\n",
    "    \"\"\"Creates a hook that stores activations in a specific dictionary.\"\"\"\n",
    "    def hook(model, input, output):\n",
    "        storage[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Create a fresh model to avoid hook accumulation\n",
    "# (registering hooks multiple times can cause issues)\n",
    "model2 = models.vgg16(pretrained=True).eval()\n",
    "\n",
    "# Register hooks for ORIGINAL image\n",
    "handles = []  # Keep track of hooks so we can remove them\n",
    "handles.append(model2.features[2].register_forward_hook(get_activation_dict(activations_original, 'early')))\n",
    "handles.append(model2.features[28].register_forward_hook(get_activation_dict(activations_original, 'deep')))\n",
    "\n",
    "# Forward pass with ORIGINAL image\n",
    "with torch.no_grad():\n",
    "    _ = model2(input_tensor)\n",
    "\n",
    "print(\"‚úÖ Captured activations for ORIGINAL image\")\n",
    "\n",
    "# Remove hooks (clean up)\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "\n",
    "# Register hooks for PERTURBED image\n",
    "handles = []\n",
    "handles.append(model2.features[2].register_forward_hook(get_activation_dict(activations_perturbed, 'early')))\n",
    "handles.append(model2.features[28].register_forward_hook(get_activation_dict(activations_perturbed, 'deep')))\n",
    "\n",
    "# Forward pass with PERTURBED image\n",
    "with torch.no_grad():\n",
    "    _ = model2(perturbed_tensor)\n",
    "\n",
    "print(\"‚úÖ Captured activations for PERTURBED image\")\n",
    "\n",
    "# Remove hooks (clean up)\n",
    "for h in handles:\n",
    "    h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Compute Difference Maps\n",
    "# ============================================\n",
    "\n",
    "# For each layer, compute: |original - perturbed|\n",
    "# This shows WHERE activations changed\n",
    "\n",
    "# Early layer difference\n",
    "# activations_original['early'] shape: [1, 64, 224, 224]\n",
    "# Step 1: Subtract ‚Üí [1, 64, 224, 224]\n",
    "# Step 2: .abs() ‚Üí absolute value (we care about magnitude of change, not direction)\n",
    "# Step 3: .mean(dim=1) ‚Üí average across all 64 channels ‚Üí [1, 224, 224]\n",
    "# Step 4: [0] ‚Üí get first (only) batch element ‚Üí [224, 224]\n",
    "\n",
    "early_diff = (activations_original['early'] - activations_perturbed['early']).abs().mean(dim=1)[0]\n",
    "deep_diff = (activations_original['deep'] - activations_perturbed['deep']).abs().mean(dim=1)[0]\n",
    "\n",
    "print(\"‚úÖ Computed difference maps\")\n",
    "print(f\"   Early layer diff shape: {early_diff.shape}\")\n",
    "print(f\"   Deep layer diff shape:  {deep_diff.shape}\")\n",
    "print(f\"\\n   Bright pixels in diff map = large activation change\")\n",
    "print(f\"   Dark pixels = little/no change\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualize Original vs Perturbed vs Differences\n",
    "# ============================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 9))\n",
    "\n",
    "# =============================\n",
    "# Row 1: The Images\n",
    "# =============================\n",
    "\n",
    "# Column 0: Original image\n",
    "axes[0, 0].imshow(img)\n",
    "axes[0, 0].set_title(\"Original Image\", fontsize=12, fontweight='bold')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Column 1: Perturbed image\n",
    "# Need to denormalize for display (reverse the normalization we did)\n",
    "# perturbed_tensor: [1, 3, 224, 224] normalized\n",
    "# Step 1: [0] ‚Üí [3, 224, 224] (remove batch)\n",
    "# Step 2: .permute(1, 2, 0) ‚Üí [224, 224, 3] (channels last for imshow)\n",
    "# Step 3: Denormalize: pixel * std + mean\n",
    "\n",
    "perturbed_display = perturbed_tensor[0].permute(1, 2, 0).numpy()\n",
    "perturbed_display = perturbed_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "perturbed_display = np.clip(perturbed_display, 0, 1)  # Clip to valid range [0, 1]\n",
    "\n",
    "axes[0, 1].imshow(perturbed_display)\n",
    "axes[0, 1].set_title(\"Perturbed Image\\n(face region blacked out)\", fontsize=12, fontweight='bold')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Column 2: Question\n",
    "axes[0, 2].text(0.5, 0.5, \"Which\\nactivations\\nchanged?\", \n",
    "                fontsize=16, ha='center', va='center', fontweight='bold')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# =============================\n",
    "# Row 2: Difference Maps\n",
    "# =============================\n",
    "\n",
    "# Column 0: Early layer difference\n",
    "axes[1, 0].imshow(early_diff.numpy(), cmap='hot')\n",
    "axes[1, 0].set_title(f\"EARLY layer difference\\n(shape: {tuple(early_diff.shape)})\", \n",
    "                     fontsize=11, fontweight='bold')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Column 1: Deep layer difference\n",
    "axes[1, 1].imshow(deep_diff.numpy(), cmap='hot')\n",
    "axes[1, 1].set_title(f\"DEEP layer difference\\n(shape: {tuple(deep_diff.shape)})\", \n",
    "                     fontsize=11, fontweight='bold')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Column 2: Observations\n",
    "axes[1, 2].text(0.05, 0.9, \"üí° Observations:\", fontsize=12, \n",
    "                transform=axes[1, 2].transAxes, fontweight='bold')\n",
    "axes[1, 2].text(0.05, 0.7, \"‚Ä¢ EARLY: Localized\\n  change at blackout\", fontsize=11, \n",
    "                transform=axes[1, 2].transAxes)\n",
    "axes[1, 2].text(0.05, 0.45, \"‚Ä¢ DEEP: Broader but\\n  still spatially\\n  structured\", fontsize=11, \n",
    "                transform=axes[1, 2].transAxes)\n",
    "axes[1, 2].text(0.05, 0.15, \"‚Üí Receptive fields\\n   grow, but spatial\\n   info persists!\", fontsize=11, \n",
    "                transform=axes[1, 2].transAxes, style='italic')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle(\"Spatial Locality: Changes in the image affect corresponding spatial locations in activations\", \n",
    "             fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üí° WHAT YOU SHOULD SEE:\")\n",
    "print(\"=\"*60)\n",
    "print(\"EARLY layer difference (bottom-left):\")\n",
    "print(\"  ‚Ä¢ A bright rectangular region\")\n",
    "print(\"  ‚Ä¢ Corresponds EXACTLY to where we blacked out pixels\")\n",
    "print(\"  ‚Ä¢ Size: ~100√ó100 (same as blackout)\")\n",
    "print(\"  ‚Ä¢ This makes sense: early layers have small receptive fields\")\n",
    "print(\"\\nDEEP layer difference (bottom-middle):\")\n",
    "print(\"  ‚Ä¢ Lower resolution (14√ó14 instead of 224√ó224)\")\n",
    "print(\"  ‚Ä¢ Bright region is BROADER (blurrier)\")\n",
    "print(\"  ‚Ä¢ But STILL in the corresponding location!\")\n",
    "print(\"  ‚Ä¢ This proves spatial structure is preserved\")\n",
    "print(\"\\n‚ö†Ô∏è KEY INSIGHT:\")\n",
    "print(\"   Even though deep layers have huge receptive fields,\")\n",
    "print(\"   they still maintain a spatial map of the image.\")\n",
    "print(\"   This is WHY Grad-CAM and saliency maps work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaway from Block 4\n",
    "\n",
    "**CNNs trade spatial precision for semantic meaning gradually.**\n",
    "\n",
    "- **Receptive fields grow** with depth (3√ó3 ‚Üí 40√ó40 ‚Üí 200√ó200)\n",
    "- **But spatial correspondence is maintained** (top-left stays top-left)\n",
    "- **This is not obvious!** It's a design property of conv layers\n",
    "\n",
    "**Why this matters:** Interpretability methods like Grad-CAM rely on this property to produce spatial heatmaps showing \"which part of the image mattered.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary: Your New Mental Model of CNNs\n",
    "\n",
    "## What We Learned\n",
    "\n",
    "### 1. Images ‚Üí Tensors ‚Üí Activations\n",
    "- **CNNs don't \"see\" images** ‚Äî they see arrays of numbers\n",
    "- **First layer transforms 3 channels ‚Üí 64 channels** ‚Äî 64 different pattern responses\n",
    "- **Feature maps ‚â† semantic understanding** ‚Äî they're local pattern detectors\n",
    "\n",
    "### 2. Convolutions = Pattern Matchers\n",
    "- **Each filter is a 3√ó3 pattern** (learned from data)\n",
    "- **Filter slides across the image** (weight sharing = efficiency)\n",
    "- **High activation = pattern found** in that location\n",
    "- **Convolution is local, not global** ‚Äî each position sees only 3√ó3 neighbors\n",
    "\n",
    "### 3. Depth Creates Abstraction\n",
    "- **Early layers:** Edges, colors (3√ó3 receptive field)\n",
    "- **Middle layers:** Textures, patterns (40√ó40 receptive field)\n",
    "- **Deep layers:** Object parts, concepts (200√ó200 receptive field)\n",
    "- **Abstraction emerges from composition**, not from any single layer being \"smart\"\n",
    "\n",
    "### 4. Spatial Structure Persists\n",
    "- **Receptive fields grow** but **spatial correspondence remains**\n",
    "- **Top-left neuron ‚Üí top-left region** of image, even in deep layers\n",
    "- **This enables spatial interpretability** (Grad-CAM, saliency, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters for Interpretability\n",
    "\n",
    "Now when you use interpretability techniques, you'll understand:\n",
    "\n",
    "| Technique | What It Does | Why It Works (Based on This Notebook) |\n",
    "|-----------|-------------|---------------------------------------|\n",
    "| **Feature Visualization** | Generate images that activate a neuron | Uses the fact that neurons detect specific patterns |\n",
    "| **Grad-CAM** | Heatmap showing important regions | Uses spatial correspondence in deep layers |\n",
    "| **Saliency Maps** | Which pixels matter most | Uses gradient flow through the network |\n",
    "| **Layer-wise Analysis** | Compare features at different depths | Uses the abstraction hierarchy (edge ‚Üí texture ‚Üí object) |\n",
    "\n",
    "---\n",
    "\n",
    "## The Foundation is Set\n",
    "\n",
    "You now have a **concrete, grounded mental model** of CNNs:\n",
    "- ‚úÖ Not \"AI magic\" ‚Äî pattern matching + composition\n",
    "- ‚úÖ Not \"semantic understanding\" ‚Äî local detectors stacked hierarchically  \n",
    "- ‚úÖ Not \"global reasoning\" ‚Äî receptive fields grow but start local\n",
    "- ‚úÖ Not \"black box\" ‚Äî we can visualize and understand each layer\n",
    "\n",
    "**Next steps:** Now we can explore interpretability techniques with this solid foundation! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
