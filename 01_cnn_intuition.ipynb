{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segment 1: Building Intuition for CNNs\n",
    "\n",
    "Before diving into interpretability techniques, we need a clear mental model of how CNNs actually work. Without this foundation, interpretability results become hard to reason about.\n",
    "\n",
    "**What we'll learn:**\n",
    "- How images become tensors and then activations\n",
    "- Convolutional filters as local pattern detectors\n",
    "- How depth creates abstraction\n",
    "- Why spatial locality matters\n",
    "\n",
    "**Key insight:** CNNs don't \"understand\" images. They detect patterns at increasingly abstract levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We'll use a pretrained VGG16 ‚Äî a simple, well-understood CNN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# Load pretrained VGG16\n",
    "model = models.vgg16(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Standard ImageNet preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Helper to load an image from URL\n",
    "def load_image(url):\n",
    "    response = requests.get(url)\n",
    "    img = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "    return img\n",
    "\n",
    "# Load a sample image (a cat)\n",
    "img_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\"\n",
    "img = load_image(img_url)\n",
    "input_tensor = preprocess(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "print(f\"Original image size: {img.size}\")\n",
    "print(f\"Input tensor shape: {input_tensor.shape}  # [batch, channels, height, width]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Code Block 1: First Contact ‚Äî Image ‚Üí Tensors ‚Üí Activations\n",
    "\n",
    "**What happens when an image enters a CNN?**\n",
    "\n",
    "1. The image becomes a tensor: `[batch, 3, 224, 224]` (3 RGB channels)\n",
    "2. The first conv layer transforms it into `[batch, 64, 224, 224]` (64 feature maps)\n",
    "3. Each feature map shows where a specific pattern was detected\n",
    "\n",
    "**Key insight:** The CNN doesn't \"see\" a cat. It sees 64 different filtered versions of the image, each highlighting different local patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations from the first conv layer\n",
    "first_conv = model.features[0]  # VGG16's first layer: Conv2d(3, 64, kernel_size=3)\n",
    "\n",
    "with torch.no_grad():\n",
    "    first_activations = first_conv(input_tensor)\n",
    "\n",
    "print(f\"Input shape:  {input_tensor.shape}   # 3 color channels\")\n",
    "print(f\"Output shape: {first_activations.shape}  # 64 feature maps\")\n",
    "\n",
    "# Visualize: original image vs first 8 feature maps\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "\n",
    "# Original image\n",
    "axes[0, 0].imshow(img)\n",
    "axes[0, 0].set_title(\"Original Image\", fontsize=10)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# First 9 feature maps\n",
    "for i in range(9):\n",
    "    row, col = (i + 1) // 5, (i + 1) % 5\n",
    "    feature_map = first_activations[0, i].numpy()\n",
    "    axes[row, col].imshow(feature_map, cmap='viridis')\n",
    "    axes[row, col].set_title(f\"Feature Map {i}\", fontsize=10)\n",
    "    axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle(\"After First Conv Layer: The image becomes 64 'filtered' versions\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Notice: Each feature map highlights different edges/textures.\")\n",
    "print(\"   The CNN immediately distorts the image into pattern-specific responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Code Block 2: Convolution as Local Pattern Detection\n",
    "\n",
    "**What are convolutional filters actually doing?**\n",
    "\n",
    "Each filter is a small `3√ó3` (or `5√ó5`) pattern that slides across the image. When the filter encounters a matching pattern, it produces a high activation.\n",
    "\n",
    "- **Same filter, different locations** ‚Üí weight sharing (efficiency)\n",
    "- **Different filters** ‚Üí detect different patterns (edges, colors, textures)\n",
    "\n",
    "**Key insight:** Convolutions are sliding pattern matchers, not global reasoning units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the learned filters from the first conv layer\n",
    "filters = first_conv.weight.data.clone()\n",
    "print(f\"Filter shape: {filters.shape}  # 64 filters, each 3x3x3 (RGB)\")\n",
    "\n",
    "# Visualize filters and their corresponding activations\n",
    "fig, axes = plt.subplots(4, 6, figsize=(15, 10))\n",
    "\n",
    "for i in range(4):\n",
    "    # Get the filter (normalize for visualization)\n",
    "    filt = filters[i].permute(1, 2, 0).numpy()  # [3, 3, 3] -> [3, 3, 3]\n",
    "    filt = (filt - filt.min()) / (filt.max() - filt.min())  # Normalize to [0,1]\n",
    "    \n",
    "    # Get corresponding activation map\n",
    "    activation = first_activations[0, i].numpy()\n",
    "    \n",
    "    # Show filter\n",
    "    axes[i, 0].imshow(filt)\n",
    "    axes[i, 0].set_title(f\"Filter {i}\\n(3√ó3 kernel)\", fontsize=9)\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Show what it detects (text annotation)\n",
    "    axes[i, 1].text(0.5, 0.5, \"‚Üí\", fontsize=30, ha='center', va='center')\n",
    "    axes[i, 1].axis('off')\n",
    "    \n",
    "    # Show activation map\n",
    "    axes[i, 2].imshow(activation, cmap='hot')\n",
    "    axes[i, 2].set_title(f\"Where filter {i}\\nactivates\", fontsize=9)\n",
    "    axes[i, 2].axis('off')\n",
    "    \n",
    "    # Highlight high activations on original\n",
    "    axes[i, 3].imshow(img)\n",
    "    # Overlay activation as alpha mask\n",
    "    activation_resized = np.array(Image.fromarray((activation * 255).astype(np.uint8)).resize(img.size))\n",
    "    axes[i, 3].imshow(activation_resized, cmap='hot', alpha=0.5)\n",
    "    axes[i, 3].set_title(\"Overlaid on image\", fontsize=9)\n",
    "    axes[i, 3].axis('off')\n",
    "    \n",
    "    # Empty cells for spacing\n",
    "    axes[i, 4].axis('off')\n",
    "    axes[i, 5].axis('off')\n",
    "\n",
    "plt.suptitle(\"Filters as Pattern Detectors: Each filter 'looks for' a specific local pattern\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key observations:\")\n",
    "print(\"   ‚Ä¢ Each 3√ó3 filter detects a specific pattern (edge direction, color gradient, etc.)\")\n",
    "print(\"   ‚Ä¢ The activation map shows WHERE that pattern appears in the image\")\n",
    "print(\"   ‚Ä¢ Same filter activates in MULTIPLE locations ‚Üí weight sharing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Code Block 3: Depth = Abstraction\n",
    "\n",
    "**The most important insight about CNNs:**\n",
    "\n",
    "As we go deeper into the network:\n",
    "- **Early layers** ‚Üí edges, colors, simple textures\n",
    "- **Middle layers** ‚Üí patterns, motifs, object parts\n",
    "- **Deep layers** ‚Üí abstract, class-relevant, sparse activations\n",
    "\n",
    "Abstraction emerges from stacking simple operations, not from any single \"smart\" layer.\n",
    "\n",
    "**Key insight:** This is why interpretability must be layer-aware!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hook to capture activations at different layers\n",
    "activations = {}\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks at early, middle, and deep layers\n",
    "# VGG16 structure: features[0-4]=block1, [5-9]=block2, [10-16]=block3, [17-23]=block4, [24-30]=block5\n",
    "model.features[2].register_forward_hook(get_activation('early'))    # After first conv block\n",
    "model.features[16].register_forward_hook(get_activation('middle'))  # Middle of network\n",
    "model.features[28].register_forward_hook(get_activation('deep'))    # Near the end\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    _ = model(input_tensor)\n",
    "\n",
    "print(\"Activation shapes at different depths:\")\n",
    "print(f\"  Early (layer 2):   {activations['early'].shape}   # 64 channels, high resolution\")\n",
    "print(f\"  Middle (layer 16): {activations['middle'].shape}  # 256 channels, medium resolution\")\n",
    "print(f\"  Deep (layer 28):   {activations['deep'].shape}    # 512 channels, low resolution\")\n",
    "\n",
    "# Visualize one channel from each depth\n",
    "fig, axes = plt.subplots(3, 5, figsize=(15, 9))\n",
    "\n",
    "for row, (name, act) in enumerate(activations.items()):\n",
    "    # Show 4 random feature maps from this layer\n",
    "    axes[row, 0].text(0.5, 0.5, f\"{name.upper()}\\nLayer\", fontsize=14, ha='center', va='center', fontweight='bold')\n",
    "    axes[row, 0].axis('off')\n",
    "    \n",
    "    # Pick 4 interesting channels (ones with high variance = more informative)\n",
    "    variances = act[0].var(dim=[1, 2])\n",
    "    top_channels = torch.argsort(variances, descending=True)[:4]\n",
    "    \n",
    "    for col, ch in enumerate(top_channels):\n",
    "        feature_map = act[0, ch].numpy()\n",
    "        axes[row, col + 1].imshow(feature_map, cmap='viridis')\n",
    "        axes[row, col + 1].set_title(f\"Channel {ch.item()}\", fontsize=9)\n",
    "        axes[row, col + 1].axis('off')\n",
    "\n",
    "# Add annotations\n",
    "axes[0, 4].text(1.1, 0.5, \"‚Üê Edges, textures\\n   (low-level)\", fontsize=10, transform=axes[0, 4].transAxes, va='center')\n",
    "axes[1, 4].text(1.1, 0.5, \"‚Üê Patterns, parts\\n   (mid-level)\", fontsize=10, transform=axes[1, 4].transAxes, va='center')\n",
    "axes[2, 4].text(1.1, 0.5, \"‚Üê Abstract, sparse\\n   (high-level)\", fontsize=10, transform=axes[2, 4].transAxes, va='center')\n",
    "\n",
    "plt.suptitle(\"Depth = Abstraction: Features become more abstract deeper in the network\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key observations:\")\n",
    "print(\"   ‚Ä¢ EARLY: High resolution, many activations, edge-like patterns\")\n",
    "print(\"   ‚Ä¢ MIDDLE: Lower resolution, more structured patterns\")\n",
    "print(\"   ‚Ä¢ DEEP: Very low resolution, sparse activations, abstract features\")\n",
    "print(\"\\n   This is why WHERE you probe the network matters for interpretability!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Code Block 4 (Optional): Spatial Locality & Receptive Fields\n",
    "\n",
    "**Why does spatial structure survive deep into the network?**\n",
    "\n",
    "Each neuron in a deep layer only \"sees\" a portion of the original image ‚Äî its **receptive field**. Deeper neurons have larger receptive fields but still maintain spatial correspondence.\n",
    "\n",
    "**Key insight:** CNNs trade spatial precision for semantic meaning gradually, not all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate: perturb a region and see which activations change\n",
    "# This shows that deep layers still have spatial structure\n",
    "\n",
    "# Create a perturbed version (black out the cat's face region)\n",
    "perturbed_tensor = input_tensor.clone()\n",
    "perturbed_tensor[:, :, 50:150, 60:160] = 0  # Black out a region\n",
    "\n",
    "# Get activations for both\n",
    "activations_original = {}\n",
    "activations_perturbed = {}\n",
    "\n",
    "def get_activation_dict(storage, name):\n",
    "    def hook(model, input, output):\n",
    "        storage[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Fresh model to avoid hook accumulation\n",
    "model2 = models.vgg16(pretrained=True).eval()\n",
    "\n",
    "# Register hooks\n",
    "handles = []\n",
    "handles.append(model2.features[2].register_forward_hook(get_activation_dict(activations_original, 'early')))\n",
    "handles.append(model2.features[28].register_forward_hook(get_activation_dict(activations_original, 'deep')))\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model2(input_tensor)\n",
    "\n",
    "# Remove and re-register for perturbed\n",
    "for h in handles:\n",
    "    h.remove()\n",
    "\n",
    "handles = []\n",
    "handles.append(model2.features[2].register_forward_hook(get_activation_dict(activations_perturbed, 'early')))\n",
    "handles.append(model2.features[28].register_forward_hook(get_activation_dict(activations_perturbed, 'deep')))\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model2(perturbed_tensor)\n",
    "\n",
    "# Compute difference maps\n",
    "early_diff = (activations_original['early'] - activations_perturbed['early']).abs().mean(dim=1)[0]\n",
    "deep_diff = (activations_original['deep'] - activations_perturbed['deep']).abs().mean(dim=1)[0]\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "\n",
    "# Row 1: Original vs Perturbed\n",
    "axes[0, 0].imshow(img)\n",
    "axes[0, 0].set_title(\"Original Image\", fontsize=11)\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Show perturbed (denormalize for display)\n",
    "perturbed_display = perturbed_tensor[0].permute(1, 2, 0).numpy()\n",
    "perturbed_display = perturbed_display * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "perturbed_display = np.clip(perturbed_display, 0, 1)\n",
    "axes[0, 1].imshow(perturbed_display)\n",
    "axes[0, 1].set_title(\"Perturbed (region blacked out)\", fontsize=11)\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "axes[0, 2].text(0.5, 0.5, \"Which activations\\nchanged?\", fontsize=14, ha='center', va='center')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Row 2: Difference maps\n",
    "axes[1, 0].imshow(early_diff.numpy(), cmap='hot')\n",
    "axes[1, 0].set_title(f\"EARLY layer difference\\n(shape: {tuple(early_diff.shape)})\", fontsize=10)\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(deep_diff.numpy(), cmap='hot')\n",
    "axes[1, 1].set_title(f\"DEEP layer difference\\n(shape: {tuple(deep_diff.shape)})\", fontsize=10)\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "axes[1, 2].text(0.1, 0.7, \"üí° Observations:\", fontsize=11, transform=axes[1, 2].transAxes, fontweight='bold')\n",
    "axes[1, 2].text(0.1, 0.5, \"‚Ä¢ Early: localized change\", fontsize=10, transform=axes[1, 2].transAxes)\n",
    "axes[1, 2].text(0.1, 0.35, \"‚Ä¢ Deep: broader but still\\n  spatially structured\", fontsize=10, transform=axes[1, 2].transAxes)\n",
    "axes[1, 2].text(0.1, 0.1, \"‚Üí Receptive fields grow\\n   but spatial info persists\", fontsize=10, transform=axes[1, 2].transAxes)\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.suptitle(\"Spatial Locality: Perturbations affect corresponding spatial locations in activations\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° This is why Grad-CAM and similar methods work:\")\n",
    "print(\"   Even deep layers maintain spatial correspondence with the input image!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: What We've Learned\n",
    "\n",
    "| Concept | What We Observed | Why It Matters for Interpretability |\n",
    "|---------|-----------------|------------------------------------|\n",
    "| **Image ‚Üí Activations** | Images become multi-channel tensors immediately | CNNs don't \"see\" objects, they see filtered responses |\n",
    "| **Filters as Detectors** | Each filter detects a specific local pattern | \"Features\" are pattern-matching, not understanding |\n",
    "| **Depth = Abstraction** | Early=edges, Middle=parts, Deep=concepts | Must choose the RIGHT layer to probe |\n",
    "| **Spatial Locality** | Perturbations affect corresponding spatial locations | Explains why saliency maps and Grad-CAM work |\n",
    "\n",
    "### Key Takeaways for Interpretability\n",
    "\n",
    "1. **Interpretability must be layer-aware** ‚Äî early and late layers show very different things\n",
    "2. **\"Features\" are emergent** ‚Äî they arise from stacking simple operations, not explicit programming\n",
    "3. **Spatial structure is preserved** ‚Äî this is why visual explanations (heatmaps) make sense\n",
    "4. **CNNs detect patterns, not semantics** ‚Äî keep this in mind when interpreting results\n",
    "\n",
    "With this mental model, we're ready to explore interpretability techniques that probe these layers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
